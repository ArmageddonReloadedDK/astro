{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.core import Activation, Dense, Dropout\n",
    "from keras.layers import concatenate,Reshape,Add,LSTM,Multiply\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential,Model\n",
    "from keras import Model\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import Input\n",
    "import keras\n",
    "import numpy as np\n",
    "import collections\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pymorphy2\n",
    "import gensim\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.array(pd.read_csv('data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2004-01-04', 'capricorn',\n",
       "       '–ù–µ –±—É–¥—å—Ç–µ —Å–ª–∏—à–∫–æ–º –Ω–∞—Å—Ç–æ–π—á–∏–≤—ã, –ø–æ–±–æ–ª—å—à–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –≤ –ø–æ—Å—Ç—É–ø–∫–∞—Ö. –î–µ–ª–æ, –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞ —Å–µ–≥–æ–¥–Ω—è, —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–∏—Ç—Å—è, –∞ –µ–≥–æ –ø–ª–æ–¥—ã –≤—ã –Ω–∞—á–Ω–µ—Ç–µ —Å–æ–±–∏—Ä–∞—Ç—å –≤ —Å–∫–æ—Ä–æ–º –±—É–¥—É—â–µ–º.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['–∏', '–≤', '–∞', '—Å', '–Ω–æ', '—É', '—è', '–∂–µ']\n"
     ]
    }
   ],
   "source": [
    "russian_stopwords = ['–∏', '–≤', '–∞', '—Å', '–Ω–æ', '—É', '—è', '–∂–µ'] #—É–∫–æ—Ä–æ—á–µ–Ω–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Ç–æ–ø—Å–ª–æ–≤\n",
    "#russian_stopwords = set(stopwords.words('russian')) #—Ä–µ–∑—É–ª—å—Ç–∞—Ç –º–æ–∂–µ—Ç —É—Ö—É–¥—à–∞—Ç—å—Å—è\n",
    "print(russian_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Å–ø–∏—Å–∫–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –æ—á–∏—â–µ–Ω–Ω—ã–π –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–ª–æ–≤ —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏ —Ä–∞–∑–±–∏—Ç—ã–π –ø–æ —Å–ª–æ–≤–∞–º:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textToList(text):\n",
    "review_lines = list()\n",
    "for line in text:\n",
    "    if line.strip() != \"\":\n",
    "        tokens = word_tokenize(line)\n",
    "        tokens = [w.lower() for w in tokens] #–ø—Ä–∏–≤–æ–¥–∏–º Case –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        words = [word for word in stripped if word.isalpha()] #—É–±–∏—Ä–∞–µ–º —Ç–æ–∫–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —è–≤–ª—è—é—Ç—Å—è —Å–∏–º–≤–æ–ª–∞–º–∏\n",
    "        words = [w for w in words if not w in russian_stopwords]\n",
    "    if (len(words) > 0):\n",
    "        review_lines.append(words)\n",
    "return review_lines  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processText(text_list):\n",
    "    tokens = [text_to_word_sequence(line, filters='!\"#$%&amp;()*+,-./:;&lt;=>?@[\\\\]^_`{|}~\\t\\n\\ufeff üè†üöóüòàü§¶ü§¶‚Äç‚ôÇÔ∏è‚ÄºÔ∏è‚ô•‚ù§Ô∏èüíóüòª',\n",
    "                                    lower=True, split=' ') for line in text_list]\n",
    "    review_lines = list()\n",
    "    for words in tokens:\n",
    "        words = [w for w in words if w.isalpha()] #—É–±–∏—Ä–∞–µ–º —Ç–æ–∫–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —è–≤–ª—è—é—Ç—Å—è —Å–∏–º–≤–æ–ª–∞–º–∏\n",
    "        words = [w for w in words if not w in russian_stopwords] #—É–±–∏—Ä–∞–µ–º —Å—Ç–æ–ø—Å–ª–æ–≤–∞\n",
    "        review_lines.append(words)\n",
    "    return review_lines  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writers_lines = processText(writers_x_train_all)\n",
    "print(writers_lines)\n",
    "print(len(writers_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ–∑–¥–∞–¥–∏–º —Å–ª–æ–π embedding, –æ–±—É—á–∏–≤ –º–æ–¥–µ–ª—å Word2Vec –Ω–∞ —Å–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "w2v=gensim.models.Word2Vec(sentences = writers_lines, size = EMBEDDING_DIM, min_count = 1, window = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(min_count=1)\n",
    "w2v.build_vocab(sentences)  # prepare the model vocabulary\n",
    "w2v.train(sentences = writers_lines, total_examples=w2v.corpus_count, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(w2v.wv.vocab)\n",
    "print(\"–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è\", len(words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_neuro",
   "language": "python",
   "name": "env_neuro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
