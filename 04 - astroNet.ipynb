{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.core import Activation, Dense, Dropout, SpatialDropout1D\n",
    "from keras.layers import concatenate,Reshape,Add,LSTM,Multiply\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers import Bidirectional, LeakyReLU\n",
    "from keras.models import Sequential,Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras import Model\n",
    "from keras import Input\n",
    "\n",
    "import cython\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "from keras.initializers import Constant\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import text_to_word_sequence,Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences,TimeseriesGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Внимание! \n",
    "\n",
    "Если планируете запускать весь ноутбук, то убедитесь, что количество озу в вашем пека >=32 гб. Либо коллаб в помощь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузим данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "размер словаря - 84267 слов,количество предложений - 73477, максимальная длинна текста - 398 символов, максимальное количество слов в предложении - 190\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.array(pd.read_csv('data.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проведем токенизацию\n",
    "\n",
    "То есть разобьем исходные предложения на слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processText(data):\n",
    "    tokens=[]\n",
    "    for line in data:\n",
    "        newToken=text_to_word_sequence(text=line[2],filters='!\"#$%&amp;()*+,-./:;&lt;=>?@[\\\\]^_`{|}~\\t\\n\\ufeff',\n",
    "                                  lower=True,split=' ')\n",
    "        tokens.append(newToken)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['любые', 'разногласия', 'во', 'мнениях', 'скоро', 'улягутся', 'а', 'вы', 'продолжайте', 'делать', 'как', 'делали', 'но', 'постарайтесь', 'не', 'наступать', 'на', 'ноги', 'слишком', 'многим', 'иначе', 'ваши', 'сегодняшние', 'действия', 'сыграют', 'против', 'вас', 'в', 'будущем']\n"
     ]
    }
   ],
   "source": [
    "wordLists=processText(data)\n",
    "print(wordLists[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заменим слова в предложениях на соответсвующие в словаре индексы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[293, 448, 66, 8385, 4715, 15933, 22, 8, 10529, 336, 34, 10057, 25, 127, 3, 16752, 7, 12640, 169, 755, 254, 43, 3549, 279, 4619, 953, 12, 2, 614]\n"
     ]
    }
   ],
   "source": [
    "num_words = 84267\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=num_words,\n",
    "    filters='!\"#$%&amp;()*+,-—./:;&lt;=>?@[\\\\]^_`{|}~\\t\\n\\xa0\\ufeff',\n",
    "    lower=True,\n",
    "    split=' ',\n",
    "    char_level=False) \n",
    "\n",
    "tokenizer.fit_on_texts(wordLists) \n",
    "sequences = np.array(tokenizer.texts_to_sequences(wordLists))\n",
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Зафиксируем характеристики выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentLen = len(max(sequences, key = len)) # max_len\n",
    "word2index = tokenizer.word_index       #word_index\n",
    "wordsNum = len(tokenizer.word_index) + 1 # num_words\n",
    "embeddingDim=300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дополним предожения нулями до одной длины"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  293   448    66  8385  4715 15933    22     8 10529   336    34 10057\n",
      "    25   127     3 16752     7 12640   169   755   254    43  3549   279\n",
      "  4619   953    12     2   614     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0]\n",
      "(190,)\n",
      "(73477, 190)\n"
     ]
    }
   ],
   "source": [
    "sequences = pad_sequences(sequences = sequences, maxlen = sentLen,padding='post')\n",
    "print(sequences[0])\n",
    "print(sequences[0].shape)\n",
    "print(sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Администратор\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "w2v = Word2Vec.load(\"word2vec.model\")\n",
    "embeddingMatrix = np.zeros((wordsNum, embeddingDim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i > wordsNum: #если индекс превышает кол-во слов в словаре, то скипаем  \n",
    "        continue\n",
    "    embeddingVector = w2v[word] #получаем вектор соответствущий слову в модели word2vec\n",
    "    if embeddingVector is not None:  #если слово отсутствует в словаре word2vec, то оно в матрице np.zeroes останется равным 0\n",
    "        embeddingMatrix[i] = embeddingVector #если слово найдено в словаре токенизатора, то в embedding_matrix проставляем вектор соответствующий слову"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Отнормируем вектора embedding'ов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler((-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "До нормализации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.20851088,  6.9667182 ,  1.03406489, -0.06461832,  1.46351445,\n",
       "       -1.40225852, -0.97575563,  1.09570146,  1.00645208,  1.2965281 ,\n",
       "        1.28454924, -2.3900702 ,  0.71035409,  0.67477763,  0.96809483,\n",
       "        0.66907966, -0.6290099 ,  1.80262721, -0.87476927, -0.26172397,\n",
       "        0.66931254,  4.15037966,  1.68157887,  0.67298663, -2.78617191,\n",
       "        1.35166252,  0.58126897, -0.9560746 , -1.56313276,  0.81927693,\n",
       "       -0.66007751, -2.74609256, -1.79234636,  1.8601526 ,  5.47511339,\n",
       "        1.59147608, -1.3512764 ,  1.73950148,  2.29833245,  0.67214918])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddingMatrix[458][:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingMatrix=scaler.fit_transform(embeddingMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После нормализации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.15737807,  0.44272599, -0.03178928,  0.08572773,  0.03299629,\n",
       "       -0.10667435,  0.05800479,  0.07516103,  0.10402152,  0.1108683 ,\n",
       "        0.04387122, -0.11529571,  0.04801753,  0.08852304,  0.1785953 ,\n",
       "        0.05145529, -0.12420372,  0.07294531, -0.14831744, -0.02330168,\n",
       "       -0.01659107,  0.17392238,  0.12742558,  0.06224952, -0.14797107,\n",
       "        0.05685835,  0.06767316, -0.0649765 , -0.11510154,  0.01222805,\n",
       "       -0.10834389, -0.15692699, -0.17532073,  0.15444414,  0.26478314,\n",
       "        0.16785802,  0.00540678,  0.12133913,  0.17657236,  0.09729453])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddingMatrix[458][:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Приведем датасет к специальному виду\n",
    "\n",
    " Например для текста \"The sky was falling due to apocalypse \" будет\n",
    " \n",
    " \"The sky\" -> was\n",
    " \n",
    " \"sky was\" -> falling\n",
    " \n",
    " \" was falling \" -> due \n",
    " \n",
    " \" falling due\" -> to\n",
    " \n",
    " \"due to \" -> apocalypse\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2idx(word):\n",
    "    return w2v.wv.vocab[word].index\n",
    "def idx2word(idx):\n",
    "    return w2v.wv.index2word[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlen=10\n",
    "step=1\n",
    "inputSent = []\n",
    "labelSent = []\n",
    "for i,line in enumerate(sequences):\n",
    "    for j in range(0, len(line) - 1, step):\n",
    "        inputSent.append(line[j])\n",
    "        labelSent.append(line[j + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[293, 448, 66, 8385, 4715]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputSent[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[448, 66, 8385, 4715]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelSent[0:4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соотнесем кадому слову из выборки Y вектор embedding'а"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Администратор\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "X=np.array(inputSent)\n",
    "Y=np.zeros((X.shape[0],embeddingDim),dtype='float32')\n",
    "\n",
    "for i,index in enumerate(labelSent):\n",
    "    Y[i]=w2v[idx2word(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=scaler.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size = 0.2,shuffle=False) #, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_train\n",
    "del y_train\n",
    "del x_test\n",
    "del y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('x_train.npy',x_train)\n",
    "np.save('y_train.npy',y_train)\n",
    "np.save('x_test.npy',x_test)\n",
    "np.save('y_test.npy',y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Раздел \"спасите, не работает\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поставлена задача по входному эмбеддингу слова определить эмбеддинг следующего слова, то есть - задача регресси. Однако по факту ничего не работает. \n",
    "\n",
    "Либо модель плохая (да), либо оптимизатор плохой (да) и все в таком духе.\n",
    "\n",
    "Варианты с GRU, SLTM не работают.\n",
    "\n",
    "Протестированы регрессионные функции потерь: косинусная метрика, среднеквадратичная "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\envs\\env_neuro\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1, 300)            25280400  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300)               19500     \n",
      "=================================================================\n",
      "Total params: 25,527,804\n",
      "Trainable params: 247,404\n",
      "Non-trainable params: 25,280,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelGRU = Sequential()\n",
    "#embedding_layer = w2v.wv.get_keras_embedding(train_embeddings=False)\n",
    "modelGRU.add(Embedding(input_dim = num_words+1, \n",
    "                              output_dim = embeddingDim, \n",
    "                              embeddings_initializer = Constant(embeddingMatrix),\n",
    "                              input_length =1, \n",
    "                              trainable = False))\n",
    "#modelGRU.add(Embedding(num_words, embedding_size))\n",
    "#modelGRU.add(SpatialDropout1D(0.2))\n",
    "#modelGRU.add(Bidirectional(GRU(40, return_sequences=True)))\n",
    "#modelGRU.add(Bidirectional(LSTM(128)))\n",
    "#modelGRU.add(LSTM(8,return_sequences=True ))\n",
    "modelGRU.add(LSTM(128))\n",
    "#modelGRU.add(Dropout(0.2))\n",
    "\n",
    "modelGRU.add(Dense(64))\n",
    "modelGRU.add(LeakyReLU(0.2))\n",
    "\n",
    "modelGRU.add(Dense(embeddingDim,activation = 'tanh'))\n",
    "modelGRU.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8887777 samples, validate on 2221945 samples\n",
      "Epoch 1/25\n",
      "8887777/8887777 [==============================] - 77s 9us/step - loss: -0.8064 - val_loss: -0.8097\n",
      "Epoch 2/25\n",
      "8887777/8887777 [==============================] - 73s 8us/step - loss: -0.8088 - val_loss: -0.8105\n",
      "Epoch 3/25\n",
      "8887777/8887777 [==============================] - 73s 8us/step - loss: -0.8099 - val_loss: -0.8111\n",
      "Epoch 4/25\n",
      "8887777/8887777 [==============================] - 72s 8us/step - loss: -0.8106 - val_loss: -0.8113\n",
      "Epoch 5/25\n",
      "2703360/8887777 [========>.....................] - ETA: 45s - loss: -0.8110"
     ]
    }
   ],
   "source": [
    "modelGRU.compile(loss=\"cosine_similarity\", optimizer='adam')\n",
    "historyGRU = modelGRU.fit(x_train, y_train, batch_size=16384, epochs=25, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_neuro",
   "language": "python",
   "name": "env_neuro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
