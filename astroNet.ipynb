{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Activation, Dense, Dropout\n",
    "from keras.layers import concatenate,Reshape,Add,LSTM,Multiply\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential,Model\n",
    "from keras import Model\n",
    "from keras import Input\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "from keras.initializers import Constant\n",
    "\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqLen=100\n",
    "dateShape=(10,)\n",
    "signShape=(10,)\n",
    "textShape=(10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDate=Input(shape=dateShape)\n",
    "denseD1=Dense(8,activation=\"relu\")(inputDate)\n",
    "outputD=Dense(16,activation=\"relu\")(denseD1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sign input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputSign=Input(shape=signShape)\n",
    "denseS1=Dense(8,activation=\"relu\")(inputSign)\n",
    "outputS=Dense(16,activation=\"relu\")(denseS1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputText=Input(shape=textShape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate date and sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined=concatenate([outputF,outputS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input->add tanh->lstm->mul sigm->lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tanh dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseTanh=Dense(100,activation='tanh')(combined)\n",
    "reshapedTanh=Reshape(textShape)(denseTanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define sigmoid dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseSigmoid=Dense(100,activation='sigmoid')(combined)\n",
    "reshapedSigmoid=Reshape(textShape)(denseSigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "add1=Add()([reshapedTanh,inputText])\n",
    "lstm1=LSTM(100)(add1)\n",
    "reshapedLstm=Reshape(textShape)(lstm1)\n",
    "mul1=Multiply()([reshapedLstm,reshapedSigmoid])\n",
    "lstm2=LSTM(100)(mul1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=Model(inputs=[inputDate,inputSign,inputText],outputs=[lstm2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "—Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è - 84267 —Å–ª–æ–≤, –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–Ω–∞ —Ç–µ–∫—Å—Ç–∞ - 398 —Å–∏–º–≤–æ–ª–æ–≤\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.array(pd.read_csv('data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processText(data):\n",
    "    tokens=[]\n",
    "    for line in data:\n",
    "        newToken=text_to_word_sequence(text=line[2],filters='!\"#$%&amp;()*+,-./:;&lt;=>?@[\\\\]^_`{|}~\\t\\n\\ufeff',\n",
    "                                  lower=True,split=' ')\n",
    "        tokens.append(newToken)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=processText(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 84267\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=num_words,\n",
    "    filters='!\"#$%&amp;()*+,-‚Äî./:;&lt;=>?@[\\\\]^_`{|}~\\t\\n\\xa0\\ufeff',\n",
    "    lower=True,\n",
    "    split=' ',\n",
    "    char_level=False) #, oov_token='unknown'\n",
    "tokenizer.fit_on_texts(tokens) #—Ñ–æ—Ä–º–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ –≤ –Ω–∞—à–µ–º —Ç–µ–∫—Å—Ç\n",
    "sequences = np.array(tokenizer.texts_to_sequences(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[293,\n",
       " 448,\n",
       " 66,\n",
       " 8385,\n",
       " 4715,\n",
       " 15933,\n",
       " 22,\n",
       " 8,\n",
       " 10529,\n",
       " 336,\n",
       " 34,\n",
       " 10057,\n",
       " 25,\n",
       " 127,\n",
       " 3,\n",
       " 16752,\n",
       " 7,\n",
       " 12640,\n",
       " 169,\n",
       " 755,\n",
       " 254,\n",
       " 43,\n",
       " 3549,\n",
       " 279,\n",
       " 4619,\n",
       " 953,\n",
       " 12,\n",
       " 2,\n",
       " 614]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeIt(review_lines):\n",
    "  num_words = 20000 #—á–∏—Å–ª–æ —Å–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤–æ–π–¥—É—Ç –≤ —Å–ª–æ–≤–∞—Ä—å —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏\n",
    "  print(\"–†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏:\", len(review_lines))\n",
    "  tokenizer = Tokenizer(num_words=num_words, filters='!\"#$%&amp;()*+,-‚Äî./:;&lt;=>?@[\\\\]^_`{|}~\\t\\n\\xa0\\ufeffüòèü§îüòüüëçüëéüèªü§òü•∫üò¶üò≤ü§™üëã‚òùÔ∏èüññ‚úåÔ∏èüëåüëèüí™üèªü§ù‚Äç‚ôÇÔ∏è‚ôÇÔ∏èüè†üöóüòàü§¶ü§¶‚Äç‚ôÇÔ∏è‚ÄºÔ∏è‚ô•‚ù§Ô∏èüíóüòªüòªüòçü§©üòÜüòéüòúüî•üò®üò±üò≠üòÇü§£üòÄüòÉüòÇüòÅüòäüòâü§óüíØ', lower=True, split=' ', char_level=False) #, oov_token='unknown'\n",
    "  tokenizer.fit_on_texts(review_lines) #—Ñ–æ—Ä–º–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ –≤ –Ω–∞—à–µ–º —Ç–µ–∫—Å—Ç\n",
    "  sequences = tokenizer.texts_to_sequences(review_lines)\n",
    "  max_len = len(max(sequences, key = len)) #–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–¥–ª–∏–Ω–∞ —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç–æ–π RNN —Å–µ—Ç–∏)\n",
    "  word_index = tokenizer.word_index\n",
    "  num_words = len(tokenizer.word_index) + 1\n",
    "  print(\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–∑—ã–≤–∞:\", max_len, \"—Å–ª–æ–≤–∞.\")\n",
    "  print(\"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤:\", len(word_index))\n",
    "  print(\"–ù–æ–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Å–ª–æ–≤–∞:\", word_index['–≤–æ–¥–∏—Ç–µ–ª—å']) #–ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–Ω–¥–µ–∫—Å –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞\n",
    "  print(\"–ö–æ–ª-–≤–æ —Å–ª–æ–≤:\", num_words)\n",
    "  return max_len, num_words, sequences, word_index, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len, num_words, sequences, word_index, tokenizer = tokenizeIt(writers_lines)\n",
    "print(word_index)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pad = pad_sequences(sequences = sequences, maxlen = max_len) #, padding='post', truncating='post') #–≤—Å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª–∏–Ω–æ–π &lt;max_len –¥–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏ –≤ –∫–æ–Ω—Ü–µ, >max_len - –æ—Ç–±—Ä–∞—Å—ã–≤–∞–µ–º\n",
    "print(x_train_pad[5])\n",
    "print(x_train_pad[5].shape)\n",
    "print(\"x_train shape:\", x_train_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º embedding —Å–ª–æ–π –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –º–æ–¥–µ–ª–∏\n",
    "#–í –º–∞—Ç—Ä–∏—Ü–µ embedding –Ω–æ–º–µ—Ä —Å–ª–æ–≤–∞ –∑–∞–º–µ–Ω—è–µ—Ç—Å—è –Ω–∞ –≤–µ–∫—Ç–æ—Ä –∏–∑ –º–æ–¥–µ–ª–∏ word2vec  \n",
    "def getEmbeddingLayer(num_words, embedding_size, max_length, tokenizer, word2vec, Trainable = False):\n",
    "  print(\"–†–∞–∑–º–µ—Ä embedding –º–∞—Ç—Ä–∏—Ü—ã:\", num_words, \"x\", embedding_size)\n",
    "  embedding_matrix = np.zeros((num_words, embedding_size))\n",
    "  for word, i in tokenizer.word_index.items():\n",
    "    if i > num_words: #–µ—Å–ª–∏ –∏–Ω–¥–µ–∫—Å –ø—Ä–µ–≤—ã—à–∞–µ—Ç –∫–æ–ª-–≤–æ —Å–ª–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä–µ, —Ç–æ —Å–∫–∏–ø–∞–µ–º  \n",
    "      continue\n",
    "    embedding_vector = w2v[word] #–ø–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—â–∏–π —Å–ª–æ–≤—É –≤ –º–æ–¥–µ–ª–∏ word2vec\n",
    "    if embedding_vector is not None:  #–µ—Å–ª–∏ —Å–ª–æ–≤–æ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ —Å–ª–æ–≤–∞—Ä–µ word2vec, —Ç–æ –æ–Ω–æ –≤ –º–∞—Ç—Ä–∏—Ü–µ np.zeroes –æ—Å—Ç–∞–Ω–µ—Ç—Å—è —Ä–∞–≤–Ω—ã–º 0\n",
    "      embedding_matrix[i] = embedding_vector #–µ—Å–ª–∏ —Å–ª–æ–≤–æ –Ω–∞–π–¥–µ–Ω–æ –≤ —Å–ª–æ–≤–∞—Ä–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞, —Ç–æ –≤ embedding_matrix –ø—Ä–æ—Å—Ç–∞–≤–ª—è–µ–º –≤–µ–∫—Ç–æ—Ä —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —Å–ª–æ–≤—É\n",
    "      embedding_layer = Embedding(input_dim = num_words, \n",
    "                              output_dim = embedding_size, \n",
    "                              embeddings_initializer = Constant(embedding_matrix),\n",
    "                              input_length = max_length, \n",
    "                              trainable = Trainable)    \n",
    "  return embedding_layer, embedding_matrix  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writers_embedding_layer, writers_embedding_matrix = getEmbeddingLayer(\n",
    "    num_words,\n",
    "    EMBEDDING_DIM,\n",
    "    max_len, \n",
    "    tokenizer,\n",
    "    writers_w2v, \n",
    "    Trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "#–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–±—É—á–∞—é—â–µ–π –∏ –ø—Ä–æ–≤–µ—Ä–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "x_train_, x_test, y_train, y_test = train_test_split(x_train_pad, y_train_all, test_size = 0.2) #, random_state = 42)\n",
    "z_train = y_train[:, num_classes:8]\n",
    "z_test = y_test[:, num_classes:8]\n",
    "y_train = y_train[:, :num_classes]\n",
    "y_test = y_test[:, :num_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(embedding_layer):\n",
    "  modelGRU = Sequential()\n",
    "  #embedding_layer = w2v.wv.get_keras_embedding(train_embeddings=False)\n",
    "  modelGRU.add(embedding_layer)\n",
    "  #modelGRU.add(Embedding(num_words, embedding_size))\n",
    "  modelGRU.add(SpatialDropout1D(0.2))\n",
    "  modelGRU.add(Bidirectional(GRU(40, return_sequences=True)))\n",
    "  modelGRU.add(Bidirectional(GRU(40)))\n",
    "  #modelGRU.add(LSTM(8,return_sequences=True ))\n",
    "  #modelGRU.add(LSTM(8))\n",
    "  modelGRU.add(Dropout(0.2))\n",
    "  modelGRU.add(Dense(64,activation = 'relu'))\n",
    "  modelGRU.add(Dropout(0.2))\n",
    "  modelGRU.add(Dense(num_classes,activation = 'sigmoid'))\n",
    "  modelGRU.summary()\n",
    "  return modelGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelGRU = buildModel(embedding_layer)\n",
    "#modelGRU.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=Adam(lr=1e-4))\n",
    "modelGRU.compile(loss='binary_crossentropy', metrics=[AUC(name='auc')], optimizer=Adam(lr=1e-4))\n",
    "historyGRU = modelGRU.fit(X_train, Y_train, batch_size=64, epochs=20, validation_data=(X_test, Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_neuro",
   "language": "python",
   "name": "env_neuro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
