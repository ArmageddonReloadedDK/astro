{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Activation, Dense, Dropout\n",
    "from keras.layers import concatenate,Reshape,Add,LSTM,Multiply\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential,Model\n",
    "from keras import Model\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import Input\n",
    "import keras\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import collections\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "from keras.initializers import Constant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqLen=100\n",
    "dateShape=(10,)\n",
    "signShape=(10,)\n",
    "textShape=(10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDate=Input(shape=dateShape)\n",
    "denseD1=Dense(8,activation=\"relu\")(inputDate)\n",
    "outputD=Dense(16,activation=\"relu\")(denseD1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sign input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputSign=Input(shape=signShape)\n",
    "denseS1=Dense(8,activation=\"relu\")(inputSign)\n",
    "outputS=Dense(16,activation=\"relu\")(denseS1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputText=Input(shape=textShape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate date and sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined=concatenate([outputF,outputS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input->add tanh->lstm->mul sigm->lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tanh dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseTanh=Dense(100,activation='tanh')(combined)\n",
    "reshapedTanh=Reshape(textShape)(denseTanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define sigmoid dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseSigmoid=Dense(100,activation='sigmoid')(combined)\n",
    "reshapedSigmoid=Reshape(textShape)(denseSigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "add1=Add()([reshapedTanh,inputText])\n",
    "lstm1=LSTM(100)(add1)\n",
    "reshapedLstm=Reshape(textShape)(lstm1)\n",
    "mul1=Multiply()([reshapedLstm,reshapedSigmoid])\n",
    "lstm2=LSTM(100)(mul1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=Model(inputs=[inputDate,inputSign,inputText],outputs=[lstm2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Подготавливаем embedding слой для использования в модели\n",
    "#В матрице embedding номер слова заменяется на вектор из модели word2vec  \n",
    "def getEmbeddingLayer(num_words, embedding_size, max_length, tokenizer, word2vec, Trainable = False):\n",
    "  print(\"Размер embedding матрицы:\", num_words, \"x\", embedding_size)\n",
    "  embedding_matrix = np.zeros((num_words, embedding_size))\n",
    "  for word, i in tokenizer.word_index.items():\n",
    "    if i > num_words: #если индекс превышает кол-во слов в словаре, то скипаем  \n",
    "      continue\n",
    "    embedding_vector = w2v[word] #получаем вектор соответствущий слову в модели word2vec\n",
    "    if embedding_vector is not None:  #если слово отсутствует в словаре word2vec, то оно в матрице np.zeroes останется равным 0\n",
    "      embedding_matrix[i] = embedding_vector #если слово найдено в словаре токенизатора, то в embedding_matrix проставляем вектор соответствующий слову\n",
    "      embedding_layer = Embedding(input_dim = num_words, \n",
    "                              output_dim = embedding_size, \n",
    "                              embeddings_initializer = Constant(embedding_matrix),\n",
    "                              input_length = max_length, \n",
    "                              trainable = Trainable)    \n",
    "  return embedding_layer, embedding_matrix  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writers_embedding_layer, writers_embedding_matrix = getEmbeddingLayer(\n",
    "    num_words,\n",
    "    EMBEDDING_DIM,\n",
    "    max_len, \n",
    "    tokenizer,\n",
    "    writers_w2v, \n",
    "    Trainable = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_neuro",
   "language": "python",
   "name": "env_neuro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
