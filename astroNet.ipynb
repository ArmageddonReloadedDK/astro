{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Activation, Dense, Dropout\n",
    "from keras.layers import concatenate,Reshape,Add,LSTM,Multiply\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential,Model\n",
    "from keras import Model\n",
    "from keras import Input\n",
    "\n",
    "import cython\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "from keras.initializers import Constant\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import text_to_word_sequence,Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences,TimeseriesGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqLen=100\n",
    "dateShape=(10,)\n",
    "signShape=(10,)\n",
    "textShape=(10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDate=Input(shape=dateShape)\n",
    "denseD1=Dense(8,activation=\"relu\")(inputDate)\n",
    "outputD=Dense(16,activation=\"relu\")(denseD1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sign input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputSign=Input(shape=signShape)\n",
    "denseS1=Dense(8,activation=\"relu\")(inputSign)\n",
    "outputS=Dense(16,activation=\"relu\")(denseS1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputText=Input(shape=textShape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate date and sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined=concatenate([outputF,outputS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input->add tanh->lstm->mul sigm->lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tanh dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseTanh=Dense(100,activation='tanh')(combined)\n",
    "reshapedTanh=Reshape(textShape)(denseTanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define sigmoid dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseSigmoid=Dense(100,activation='sigmoid')(combined)\n",
    "reshapedSigmoid=Reshape(textShape)(denseSigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add1=Add()([reshapedTanh,inputText])\n",
    "lstm1=LSTM(100)(add1)\n",
    "reshapedLstm=Reshape(textShape)(lstm1)\n",
    "mul1=Multiply()([reshapedLstm,reshapedSigmoid])\n",
    "lstm2=LSTM(100)(mul1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=Model(inputs=[inputDate,inputSign,inputText],outputs=[lstm2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузим данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "размер словаря - 84267 слов,количество предложений - 73477, максимальная длинна текста - 398 символов, максимальное количество слов в предложении - 190\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.array(pd.read_csv('data.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проведем токенизацию\n",
    "\n",
    "То есть разобьем исходные предложения на слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processText(data):\n",
    "    tokens=[]\n",
    "    for line in data:\n",
    "        newToken=text_to_word_sequence(text=line[2],filters='!\"#$%&amp;()*+,-./:;&lt;=>?@[\\\\]^_`{|}~\\t\\n\\ufeff',\n",
    "                                  lower=True,split=' ')\n",
    "        tokens.append(newToken)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['любые', 'разногласия', 'во', 'мнениях', 'скоро', 'улягутся', 'а', 'вы', 'продолжайте', 'делать', 'как', 'делали', 'но', 'постарайтесь', 'не', 'наступать', 'на', 'ноги', 'слишком', 'многим', 'иначе', 'ваши', 'сегодняшние', 'действия', 'сыграют', 'против', 'вас', 'в', 'будущем']\n"
     ]
    }
   ],
   "source": [
    "wordLists=processText(data)\n",
    "print(wordLists[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заменим слова в предложениях на соответсвующие в словаре индексы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[293, 448, 66, 8385, 4715, 15933, 22, 8, 10529, 336, 34, 10057, 25, 127, 3, 16752, 7, 12640, 169, 755, 254, 43, 3549, 279, 4619, 953, 12, 2, 614]\n"
     ]
    }
   ],
   "source": [
    "num_words = 84267\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=num_words,\n",
    "    filters='!\"#$%&amp;()*+,-—./:;&lt;=>?@[\\\\]^_`{|}~\\t\\n\\xa0\\ufeff',\n",
    "    lower=True,\n",
    "    split=' ',\n",
    "    char_level=False) \n",
    "\n",
    "tokenizer.fit_on_texts(wordLists) \n",
    "sequences = np.array(tokenizer.texts_to_sequences(wordLists))\n",
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Зафиксируем характеристики выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentLen = len(max(sequences, key = len)) # max_len\n",
    "word2index = tokenizer.word_index       #word_index\n",
    "wordsNum = len(tokenizer.word_index) + 1 # num_words\n",
    "embeddingDim=300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дополним предожения нулями до одной длины"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  293   448    66  8385  4715 15933    22     8 10529   336    34 10057\n",
      "    25   127     3 16752     7 12640   169   755   254    43  3549   279\n",
      "  4619   953    12     2   614     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0]\n",
      "(190,)\n",
      "(73477, 190)\n"
     ]
    }
   ],
   "source": [
    "sequences = pad_sequences(sequences = sequences, maxlen = sentLen,padding='post')\n",
    "print(sequences[0])\n",
    "print(sequences[0].shape)\n",
    "print(sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Администратор\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "w2v = Word2Vec.load(\"word2vec.model\")\n",
    "embeddingMatrix = np.zeros((wordsNum, embeddingDim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i > wordsNum: #если индекс превышает кол-во слов в словаре, то скипаем  \n",
    "        continue\n",
    "    embeddingVector = w2v[word] #получаем вектор соответствущий слову в модели word2vec\n",
    "    if embeddingVector is not None:  #если слово отсутствует в словаре word2vec, то оно в матрице np.zeroes останется равным 0\n",
    "        embeddingMatrix[i] = embeddingVector #если слово найдено в словаре токенизатора, то в embedding_matrix проставляем вектор соответствующий слову"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "std=embeddingMatrix.std(axis=0)\n",
    "mean=embeddingMatrix.mean(axis=0)\n",
    "for i in range(embeddingMatrix.shape[0]):\n",
    "    for j in range(embeddingMatrix.shape[1]):\n",
    "        embeddingMatrix[i][j]=(embeddingMatrix[i][j]-mean[j])/std[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingLayer = Embedding(\n",
    "    input_dim = wordsNum, \n",
    "    output_dim = embeddingDim, \n",
    "    embeddings_initializer = Constant(embeddingMatrix),\n",
    "    input_length = sentLen, \n",
    "    trainable = False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Приведем датасет к специальному виду\n",
    "\n",
    " Например для текста \"The sky was falling due to apocalypse \" будет\n",
    " \n",
    " \"The sky\" -> was\n",
    " \n",
    " \"sky was\" -> falling\n",
    " \n",
    " \" was falling \" -> due \n",
    " \n",
    " \" falling due\" -> to\n",
    " \n",
    " \"due to \" -> apocalypse\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2idx(word):\n",
    "    return w2v.wv.vocab[word].index\n",
    "def idx2word(idx):\n",
    "    return w2v.wv.index2word[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlen=10\n",
    "step=1\n",
    "inputSent = []\n",
    "labelSent = []\n",
    "for i,line in enumerate(sequences):\n",
    "    for j in range(0, len(line) - seqlen, step):\n",
    "        inputSent.append(line[j:j + seqlen])\n",
    "        labelSent.append(line[j + seqlen])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Администратор\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "X=np.array(inputSent)\n",
    "Y=np.zeros((X.shape[0],embeddingDim),dtype='float32')\n",
    "\n",
    "for i,index in enumerate(labelSent):\n",
    "    Y[i]=w2v[idx2word(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13225860, 300)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.7566195 , -0.16491489, -0.38569096,  0.16882984,  0.8041008 ,\n",
       "       -0.16769701, -0.9228554 , -2.150708  ,  0.28348798,  1.2643976 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "del embeddingMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Y.npy',Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=np.load('Y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-6a1c51b67f07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for j in range(Y.shape[1]):\n",
    "    Y[:,j]=(Y[:,j]-mean[j])/std[j]\n",
    "    if j%1000==0:\n",
    "        print(j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_, x_test, y_train, y_test = train_test_split(X,Y, test_size = 0.2) #, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер embedding матрицы: 84267 x 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Администратор\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Размер embedding матрицы:\", num_words, \"x\", embeddingDim)\n",
    "embeddingMatrix = np.zeros((num_words+1, embeddingDim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i > num_words: #если индекс превышает кол-во слов в словаре, то скипаем  \n",
    "        continue\n",
    "    embeddingVector = w2v[word] #получаем вектор соответствущий слову в модели word2vec\n",
    "    if embeddingVector is not None:  #если слово отсутствует в словаре word2vec, то оно в матрице np.zeroes останется равным 0\n",
    "        embeddingMatrix[i] = embeddingVector #если слово найдено в словаре токенизатора, то в embedding_matrix проставляем вектор соответствующий слову\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingLayer = Embedding(input_dim = num_words, \n",
    "                              output_dim = embeddingDim, \n",
    "                              embeddings_initializer = Constant(embeddingMatrix),\n",
    "                              input_length =sentLen, \n",
    "                              trainable = False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelGRU = Sequential()\n",
    "#embedding_layer = w2v.wv.get_keras_embedding(train_embeddings=False)\n",
    "modelGRU.add(embeddingLayer)\n",
    "#modelGRU.add(Embedding(num_words, embedding_size))\n",
    "modelGRU.add(SpatialDropout1D(0.2))\n",
    "modelGRU.add(Bidirectional(GRU(40, return_sequences=True)))\n",
    "modelGRU.add(Bidirectional(GRU(40)))\n",
    "#modelGRU.add(LSTM(8,return_sequences=True ))\n",
    "#modelGRU.add(LSTM(8))\n",
    "modelGRU.add(Dropout(0.2))\n",
    "modelGRU.add(Dense(64,activation = 'relu'))\n",
    "modelGRU.add(Dropout(0.2))\n",
    "modelGRU.add(Dense(num_classes,activation = 'sigmoid'))\n",
    "modelGRU.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelGRU = buildModel(embedding_layer)\n",
    "#modelGRU.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=Adam(lr=1e-4))\n",
    "modelGRU.compile(loss='binary_crossentropy', metrics=[AUC(name='auc')], optimizer=Adam(lr=1e-4))\n",
    "historyGRU = modelGRU.fit(X_train, Y_train, batch_size=64, epochs=20, validation_data=(X_test, Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_neuro",
   "language": "python",
   "name": "env_neuro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
