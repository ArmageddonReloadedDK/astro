{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import nest_asyncio\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from loguru import logger\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "import time\n",
    "from fake_useragent import UserAgent\n",
    "from collections import Counter\n",
    "import urllib\n",
    "\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Спарсим рамблер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signs = ['aries', 'gemini', 'taurus', 'cancer', 'leo', 'virgo', 'libra', 'scorpio', 'sagittarius', 'capricorn',\n",
    "         'aquarius', 'pisces']\n",
    "data = pd.read_csv('data.csv', names=['date', 'sign', 'text'])\n",
    "\n",
    "for i in range(2004, 2021):\n",
    "    for j in range(1, 13):\n",
    "        for k in range(1, 32):\n",
    "            for sign in signs:\n",
    "                try:\n",
    "                    year = i\n",
    "                    month = j\n",
    "                    day = k\n",
    "                    if month < 10:\n",
    "                        month = '0' + str(month)\n",
    "                    if day < 10:\n",
    "                        day = '0' + str(day)\n",
    "                    date = '%s-%s-%s' % (year, month, day)\n",
    "                    url = 'https://horoscopes.rambler.ru/%s/%s/?updated' % (sign, date)  # url для второй страницы\n",
    "                    print(url)\n",
    "                    r = requests.get(url)\n",
    "                    response = r.text.encode('utf-8')\n",
    "\n",
    "                    soup = BeautifulSoup(response, features=\"lxml\")\n",
    "                    text = soup.find('div', {'class': '_1dQ3'}).text\n",
    "                    data = data.append({'date': date, 'sign': sign, 'text': text}, ignore_index=True)\n",
    "                    data.to_csv('data.csv', encoding='utf-8')\n",
    "                    print('saved date=', date, ' sign=', sign)\n",
    "                except Exception:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сайт Ростова-на-Дону"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.1rnd.ru/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = []\n",
    "url = 'https://www.1rnd.ru/news/3089599'\n",
    "print(url)\n",
    "r = requests.get(url)\n",
    "response = r.text\n",
    "soup = BeautifulSoup(response, 'html.parser')\n",
    "\n",
    "news_text = soup.findAll('p', {'style': \"text-align:justify;\"})\n",
    "date_text = soup.findAll('div', {'class': \"article-info__time\"})\n",
    "header_text = soup.findAll('div', {'class': 'title-container inner-title'})\n",
    "\n",
    "if news_text:\n",
    "    print('success')\n",
    "    text_line = ''\n",
    "    for text in news_text:\n",
    "        text_line = text_line + text.get_text()\n",
    "    news.append([header_text[0].get_text(), date_text[0].get_text().split(), text_line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.DataFrame({'header': [0], 'date': [0], 'news': [0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start1 = 7135\n",
    "start2 = 300000\n",
    "start3 = 600000\n",
    "start4 = 900000\n",
    "start5 = 1200000\n",
    "start6 = 1500000\n",
    "start7 = 1800000\n",
    "start8 = 2100000\n",
    "start9 = 2400000\n",
    "start10 = 2700000\n",
    "\n",
    "end1 = 299999\n",
    "end2 = 599999\n",
    "end3 = 899999\n",
    "end4 = 1199999\n",
    "end5 = 1499999\n",
    "end6 = 1799999\n",
    "end7 = 2099999\n",
    "end8 = 2399999\n",
    "end9 = 2699999\n",
    "end10 = 2999999\n",
    "\n",
    "\n",
    "async def get_news(start, end):\n",
    "    global news\n",
    "    for i in range(start, end):\n",
    "        url = 'https://www.1rnd.ru/news/{0}'.format(str(i))\n",
    "        print(url)\n",
    "\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.get(url) as response:\n",
    "\n",
    "                response = await response.text()\n",
    "                soup = BeautifulSoup(response, 'html.parser')\n",
    "\n",
    "                news_text = soup.findAll('p', {'style': \"text-align:justify;\"})\n",
    "                date_text = soup.findAll('div', {'class': \"article-info__time\"})\n",
    "                header_text = soup.findAll('div', {'class': 'title-container inner-title'})\n",
    "\n",
    "                if news_text:\n",
    "                    print('success')\n",
    "                    text_line = ''\n",
    "                    for text in news_text:\n",
    "                        text_line = text_line + text.get_text()\n",
    "\n",
    "                    date = date_text[0].get_text()\n",
    "                    header = header_text[0].get_text()\n",
    "\n",
    "                    news = news.append({'header': header, 'date': date, 'news': text_line}, ignore_index=True)\n",
    "                    news.to_csv('news.csv', encoding='utf-8')\n",
    "\n",
    "\n",
    "futures = [get_news(start1, end1),\n",
    "           get_news(start2, end2),\n",
    "           get_news(start3, end3),\n",
    "           get_news(start4, end4),\n",
    "           get_news(start5, end5),\n",
    "           get_news(start6, end6),\n",
    "           get_news(start7, end7),\n",
    "           get_news(start8, end8),\n",
    "           get_news(start9, end9),\n",
    "           get_news(start10, end10)]\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(asyncio.wait(futures))\n",
    "loop.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Товарная единица - ивент\n",
    "\n",
    "Нужно определить - \n",
    " - [x] категория\n",
    " - [x] стоимость \n",
    " - [x] геопозиция\n",
    " - [x] дата\n",
    " - [x] длительность\n",
    " - [x] количество людей\n",
    " - [ ] список услуг (недоступно)\n",
    " - [x] кто проводит\n",
    " - [x] сайт"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# BookYogaRetreats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "page = 1\n",
    "base_url = 'www.bookyogaretreats.com'\n",
    "url = f'https://{base_url}/?page={page}'\n",
    "print(url)\n",
    "r = requests.get(url)\n",
    "response = r.text.encode('utf-8')\n",
    "\n",
    "soup = BeautifulSoup(response)\n",
    "text = soup.findAll('a', {'class': 'js-showcard-link'}, href=True)\n",
    "card_urls = []\n",
    "for t in text:\n",
    "    card_urls.append(t['href'])\n",
    "# data=data.append({'date': date, 'sign': sign, 'text': text}, ignore_index=True)\n",
    "# data.to_csv('data.csv', encoding = 'utf-8')\n",
    "# print('saved date=', date, ' sign=', sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = 1\n",
    "base_url = 'www.bookyogaretreats.com'\n",
    "url = f'https://{base_url}{card_urls[10]}'\n",
    "# url='https://www.bookyogaretreats.com/yoga-weeks/6-day-urban-yoga-retreat-in-barcelona-catalonia'\n",
    "print(url)\n",
    "r = requests.get(url)\n",
    "response = r.text.encode('utf-8')\n",
    "\n",
    "soup = BeautifulSoup(response, 'html.parser')\n",
    "text = soup.findAll('a', {'class': 'js-showcard-link'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.findAll('div', {'class': 'pre-main-content__location'})[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.findAll('div', {'class': 'listing-title'})[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = soup.findAll('div', {\n",
    "    'class': 'subtitle-small listing-query-box__duration js-sc3-inquiry-modal-duration js-insert-duration'})[0].text\n",
    "duration = duration.replace('\\n', '').split(' ')[0]\n",
    "duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = soup.findAll('h1', {'class': 'listing-title__title title listing-title-new'})[0].text\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrival = soup.findAll('span', {'class': 'js-sc1-inquiry-modal-arrival'})\n",
    "if len(arrival) != 0:\n",
    "    arrival = arrival[0].text\n",
    "    arrival = arrival.replace(',', '').split(' ')\n",
    "    arrival[1] = arrival[1][:3]\n",
    "    arrival = ' '.join(arrival[1:])\n",
    "    arrival = datetime.strptime(arrival, '%b %d %Y').date()\n",
    "else:\n",
    "    arrival = None\n",
    "\n",
    "print(arrival)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "departure = soup.findAll('span', {'class': 'js-sc1-inquiry-modal-departure'})\n",
    "if len(departure) != 0:\n",
    "    departure = departure[0].text\n",
    "    departure = departure.replace(',', '').split(' ')\n",
    "    departure[1] = departure[1][:3]\n",
    "    departure = ' '.join(departure[1:])\n",
    "    departure = datetime.strptime(departure, '%b %d %Y').date()\n",
    "else:\n",
    "    departure = None\n",
    "\n",
    "print(departure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.strptime('May 17 2022', '%b %d %Y').date().isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organizer = soup.findAll('div', {'class': 'listing-organizer-card__content'})[0].text[20:].split('\\n')[0]\n",
    "organizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = soup.findAll('div', {'class': 'listing-overview__notes'})[0].text.split('\\n')\n",
    "arr = [a for a in arr if a != '']\n",
    "duration = arr[0].split(' ')[0]\n",
    "duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = soup.findAll('div', {'class': 'listing-title__location listing-location-new'})\n",
    "if len(location[0]) != 0:\n",
    "    location = location[0].text\n",
    "else:\n",
    "    location = None\n",
    "\n",
    "print(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = soup.findAll('div', {'class': 'listing-overview__notes'})[0].text.split('\\n')\n",
    "arr = [a for a in arr if a != '']\n",
    "group_size = [a for a in arr if 'Group' in a]\n",
    "\n",
    "if len(group_size) != 0:\n",
    "    if len(group_size[0].split(' ')) == 6:\n",
    "        group_size = group_size[0].split(' ')[4]\n",
    "    elif len(group_size[0].split(' ')) == 4:\n",
    "        group_size = group_size[0].split(' ')[2]\n",
    "else:\n",
    "    group_size = None\n",
    "\n",
    "print(arr)\n",
    "print(group_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox(executable_path=r\"D:\\загрузки\\geckodriver-v0.30.0-win64\\geckodriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)\n",
    "driver.find_elements_by_class_name('price')[0].text.split(' ')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main page\n",
    "\n",
    "main_page_url = 'https://www.bookyogaretreats.com'\n",
    "r = requests.get(main_page_url)\n",
    "response = r.text.encode('utf-8')\n",
    "\n",
    "soup = BeautifulSoup(response)\n",
    "\n",
    "categories_raw = soup.findAll('li', {'class': 'fakeCheck'})\n",
    "categories_raw = [' '.join(category.text.replace('\\n', '').split(' ')[:-1]) for category in categories_raw][12:-4]\n",
    "categories = []\n",
    "\n",
    "for i, category in enumerate(categories_raw):\n",
    "    if 'level' in category.lower():\n",
    "        categories.append(category.split(' ')[1])\n",
    "    else:\n",
    "        categories.append(category)\n",
    "\n",
    "# categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.findAll('div', {'id': 'error-page'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'https://www.bookyogaretreats.com/sr?c=budget-retreats&page=1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'title': [0], 'duration': [0], 'arrival': [0], 'departure': [0], 'organizer': [0], 'location': [0],\n",
    "                     'group_size': [0], 'price': [0]})\n",
    "# data_categories={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_page_url = 'https://www.bookyogaretreats.com'\n",
    "\n",
    "ua = UserAgent(verify_ssl=False)\n",
    "headers = {'User-Agent': ua.random}\n",
    "\n",
    "r = requests.get(main_page_url, headers=headers)\n",
    "response = r.text.encode('utf-8')\n",
    "soup = BeautifulSoup(response)\n",
    "\n",
    "categories_raw = soup.findAll('li', {'class': 'fakeCheck'})\n",
    "categories_raw = [' '.join(category.text.replace('\\n', '').split(' ')[:-1]) for category in categories_raw][12:-4]\n",
    "categories = []\n",
    "\n",
    "driver = webdriver.Firefox(executable_path=r\"D:\\загрузки\\geckodriver-v0.30.0-win64\\geckodriver.exe\")\n",
    "\n",
    "for i, category in enumerate(categories_raw):\n",
    "    if 'level' in category.lower():\n",
    "        categories.append(category.split(' ')[1])\n",
    "    else:\n",
    "        categories.append(category)\n",
    "\n",
    "# c_u=category_page_url=main_page_url+f'/sr?c={cat}&page={page}'\n",
    "\n",
    "for page in range(428, 429):\n",
    "    #     cat='-'.join(category.split(' ')).lower()\n",
    "    cat = 'cat '\n",
    "    #     stop=False\n",
    "\n",
    "    #     data_categories[cat]=[]\n",
    "    #     while stop is False:\n",
    "\n",
    "    page_url = main_page_url + f'/?page={page}'\n",
    "    print('NEW Page ', page_url, '\\n----------------------')\n",
    "\n",
    "    # страница каталога \n",
    "    r = requests.get(page_url, headers=headers)\n",
    "    response = r.text.encode('utf-8')\n",
    "    soup = BeautifulSoup(response)\n",
    "\n",
    "    # если такой страницы нет, т.е. каталог закончился\n",
    "    error = soup.findAll('div', {'id': 'error-page'})\n",
    "\n",
    "    if len(error) > 0:\n",
    "        stop = True\n",
    "        print(f'####Category page {page} ERROR####')\n",
    "        break\n",
    "\n",
    "    text = soup.findAll('a', {'class': 'js-showcard-link'}, href=True)\n",
    "    card_urls = []\n",
    "    for t in text:\n",
    "        card_urls.append(t['href'])\n",
    "\n",
    "    for card_url in card_urls:\n",
    "\n",
    "        url = f'{main_page_url}{card_url}'\n",
    "        print(f'category {cat}, url {url}')\n",
    "        r = requests.get(url, headers=headers)\n",
    "        response = r.text.encode('utf-8')\n",
    "        soup = BeautifulSoup(response, 'html.parser')\n",
    "\n",
    "        error = soup.findAll('div', {'id': 'error-page'})\n",
    "        if len(error) > 0:\n",
    "            continue\n",
    "\n",
    "        arr = soup.findAll('div', {'class': 'listing-overview__notes'})[0].text.split('\\n')\n",
    "        arr = [a for a in arr if a != '']\n",
    "        duration = arr[0].split(' ')[0]\n",
    "\n",
    "        title = soup.findAll('h1', {'class': 'listing-title__title title listing-title-new'})[0].text\n",
    "        #             data_categories[cat].append(title)\n",
    "\n",
    "        arrival = soup.findAll('span', {'class': 'js-sc1-inquiry-modal-arrival'})\n",
    "        if len(arrival) != 0:\n",
    "            arrival = arrival[0].text\n",
    "            arrival = arrival.replace(',', '').split(' ')\n",
    "            arrival[1] = arrival[1][:3]\n",
    "            arrival = ' '.join(arrival[1:])\n",
    "            arrival = datetime.strptime(arrival, '%b %d %Y').date()\n",
    "        else:\n",
    "            arrival = None\n",
    "\n",
    "        departure = soup.findAll('span', {'class': 'js-sc1-inquiry-modal-departure'})\n",
    "        if len(departure) != 0:\n",
    "            departure = departure[0].text\n",
    "            departure = departure.replace(',', '').split(' ')\n",
    "            departure[1] = departure[1][:3]\n",
    "            departure = ' '.join(departure[1:])\n",
    "            departure = datetime.strptime(departure, '%b %d %Y').date()\n",
    "        else:\n",
    "            departure = None\n",
    "\n",
    "        organizer = soup.findAll('div', {'class': 'listing-organizer-card__content'})[0].text[20:].split('\\n')[0]\n",
    "\n",
    "        location = soup.findAll('div', {'class': 'listing-title__location listing-location-new'})\n",
    "        if len(location[0]) != 0:\n",
    "            location = location[0].text\n",
    "        else:\n",
    "            location = None\n",
    "\n",
    "        arr = soup.findAll('div', {'class': 'listing-overview__notes'})[0].text.split('\\n')\n",
    "        arr = [a for a in arr if a != '']\n",
    "        group_size = [a for a in arr if 'Group' in a]\n",
    "\n",
    "        if len(group_size) != 0:\n",
    "            if len(group_size[0].split(' ')) == 6:\n",
    "                group_size = group_size[0].split(' ')[4]\n",
    "            elif len(group_size[0].split(' ')) == 4:\n",
    "                group_size = group_size[0].split(' ')[2]\n",
    "        else:\n",
    "            group_size = None\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        price_list = driver.find_elements_by_class_name('price')\n",
    "        if len(price_list) > 0:\n",
    "            if len(price_list[0].text.split(' ')) < 2:\n",
    "                price = None\n",
    "            else:\n",
    "                price = price_list[0].text.split(' ')[1]\n",
    "        else:\n",
    "            price = None\n",
    "\n",
    "        data = data.append({'title': [title], 'duration': [duration], 'arrival': [arrival], 'departure': [departure],\n",
    "                            'organizer': [organizer], 'location': [location], 'group_size': [group_size],\n",
    "                            'price': [price]}, ignore_index=True)\n",
    "    data.to_csv('data.csv', encoding='utf-8')\n",
    "#         np.save('data_categories.npy',data_categories)\n",
    "\n",
    "#         page+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Только страны"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_countries = pd.DataFrame({'title': [0], 'country': [0]})\n",
    "# data_categories={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_page_url = 'https://www.bookyogaretreats.com'\n",
    "\n",
    "ua = UserAgent(verify_ssl=False)\n",
    "headers = {'User-Agent': ua.random}\n",
    "\n",
    "for page in range(0, 429):\n",
    "\n",
    "    cat = 'cat '\n",
    "\n",
    "    page_url = main_page_url + f'/?page={page}'\n",
    "    print('NEW Page ', page_url, '\\n----------------------')\n",
    "\n",
    "    # страница каталога \n",
    "    r = requests.get(page_url, headers=headers)\n",
    "    response = r.text.encode('utf-8')\n",
    "    soup = BeautifulSoup(response)\n",
    "\n",
    "    countries = [country.text for country in soup.findAll('div', {'class': 'pre-main-content__location'})]\n",
    "    titles = [title.text for title in soup.findAll('div', {'class': 'listing-title'})]\n",
    "\n",
    "    for i, title in enumerate(titles):\n",
    "        data_countries = data_countries.append({'title': title[1:-1], 'country': countries[i][1:-1]}, ignore_index=True)\n",
    "    data_countries.to_csv('data_countries.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_countries = data_countries.drop_duplicates()\n",
    "cleaned_countries = cleaned_countries.drop(cleaned_countries.index[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_countries.head()\n",
    "cleaned_countries.to_csv('cleaned_countries.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in cleaned_countries.values:\n",
    "    print(line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = '-'.join(categories[0].split(' '))\n",
    "category_page_url = main_page_url + f'/sr?c={cat}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paths = glob.glob('byr/*')\n",
    "dfs = [pd.read_csv(path) for path in df_paths]\n",
    "data = pd.concat(dfs)\n",
    "del data['Unnamed: 0']\n",
    "data_df = data.drop_duplicates()\n",
    "data = data_df.values[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paths = glob.glob('byr/*')\n",
    "dfs = [pd.read_csv(path) for path in df_paths]\n",
    "data = pd.concat(dfs)\n",
    "del data['Unnamed: 0']\n",
    "data_df = data.drop_duplicates()\n",
    "data = data_df.values[1:]\n",
    "\n",
    "for i, line in enumerate(data):\n",
    "\n",
    "    title, duration, arrival, departure, organizer, location, group_size, price = line\n",
    "    title = title[2:-2]\n",
    "    duration = float(duration[2:-2])\n",
    "\n",
    "    if len(arrival) > 6:\n",
    "        arrival = arrival[15:-2].replace(',', '').replace(' ', '-')\n",
    "    else:\n",
    "        arrival = None\n",
    "\n",
    "    if len(departure) > 6:\n",
    "        departure = departure[15:-2].replace(',', '').replace(' ', '-')\n",
    "    else:\n",
    "        departure = None\n",
    "\n",
    "    organizer = organizer[2:-2]\n",
    "    location = location[2:-2]\n",
    "\n",
    "    if 'None' in group_size:\n",
    "        group_size = None\n",
    "    elif '-' in group_size:\n",
    "        group_size = group_size[2:-2].split('-')[1]\n",
    "        group_size = float(group_size)\n",
    "    else:\n",
    "        group_size = float(group_size[2:-2])\n",
    "\n",
    "    if 'None' in price:\n",
    "        price = None\n",
    "    else:\n",
    "        price = price[2:-2].replace(',', '')\n",
    "        price = float(price)\n",
    "\n",
    "    data[i] = [title, duration, arrival, departure, organizer, location, group_size, price]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = pd.DataFrame(\n",
    "    {'title': data[:, 0], 'duration': data[:, 1], 'arrival': data[:, 2], 'departure': data[:, 3],\n",
    "     'organizer': data[:, 4], 'location': data[:, 5], 'group_size': data[:, 6], 'price': data[:, 7]})\n",
    "cleaned_data.to_csv('cleaned_data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# количество ретритов по годам и без даты\n",
    "\n",
    "step = 1\n",
    "year = '2022'\n",
    "for line in cleaned_data.values:\n",
    "    # if line[2] is None:\n",
    "    if line[2][:4]==year:\n",
    "        step += 1\n",
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data['price'].astype(float).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# объем рынка\n",
    "\n",
    "total = 0\n",
    "for line in cleaned_data.values:\n",
    "    price = line[-1]\n",
    "    group_size = line[-2]\n",
    "    if price is not None and group_size is not None:\n",
    "        total += price * group_size\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# количество уникальных ретитов по датам\n",
    "month_list=['01','02','03','04','05','06','07','08','09','10','11','12']\n",
    "\n",
    "for month_comp in month_list:\n",
    "    titles = []\n",
    "    for line in cleaned_data.values:\n",
    "        if line[2] is not None:\n",
    "            year, month, day = line[2].split('-')\n",
    "            title = line[0]\n",
    "            if str(month_comp) == month:\n",
    "                titles.append(title)\n",
    "\n",
    "    month_df = pd.DataFrame({'titles': titles}).drop_duplicates()\n",
    "\n",
    "    print(len(month_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# количество уникальных ретитов по годам\n",
    "year_list=['2022','2023','2024']\n",
    "\n",
    "for year_comp in year_list:\n",
    "    titles = []\n",
    "    for line in cleaned_data.values:\n",
    "        if line[2] is not None:\n",
    "            year, month, day = line[2].split('-')\n",
    "            title = line[0]\n",
    "            # if str(month_comp) == month:\n",
    "            if year_comp == year:\n",
    "                titles.append(title)\n",
    "\n",
    "    month_df = pd.DataFrame({'titles': titles}).drop_duplicates()\n",
    "\n",
    "    print(len(month_df))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество ретритов на сайте (шт) - 4650\n",
    "\n",
    "Количество ретритов с привязкой к дате (шт): 2022г - 1437, 2023г - 36, 2024г - 2\n",
    "\n",
    "Количество ретритов без привязки к дате (шт): 3181\n",
    "\n",
    "Количество уникальных ретритов с привязкой по дате по месяцам (шт): январь - 13, февраль - 64, март - 238, апрель - 246, май - 212, июнь - 183, июль - 143, август - 101, сентябрь - 99, ноябрь - 56, декабрь - 28 \n",
    "\n",
    "\n",
    "\n",
    "Цена (евро): среднее - 1243, медиана - 900,  дисперсия - 1190, миниум - 18, максимум - 27300 \n",
    "\n",
    "Длительность (дни): среднее - 10, медиана - 6, дисперсия - 16, минимум - 1, максимум 365\n",
    "\n",
    "Количество человек (шт): среднее - 13.5, медиана - 12, дисперсия - 12, минимум - 1, максимум 500\n",
    "\n",
    "Сумма стоимости ретритов х количество участников (млн евро) - 56.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = pd.read_csv('bookyogaretreat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contries = []\n",
    "for line in cleaned_data.values:\n",
    "    cnt = line[6]\n",
    "    contries.append(cnt)\n",
    "contries.sort()\n",
    "#144\n",
    "contries = contries[0:]\n",
    "contries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = contries\n",
    "counts = Counter(values)\n",
    "contries_x = []\n",
    "contries_y = []\n",
    "for k, count in counts.most_common():\n",
    "    contries_x.append(k)\n",
    "    contries_y.append(count)\n",
    "    print(k, count * 'x')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30\n",
    "plt.figure(figsize=(30, 15))\n",
    "plt.plot(contries_x[:N], contries_y[:N])\n",
    "plt.yticks(np.arange(0, 450, 25))\n",
    "plt.grid(True)\n",
    "plt.legend(['количество ретритов за все время по странам'], fontsize=20)\n",
    "# plt.savefig('ретриты по странам.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for line in contries_y:\n",
    "for line in contries_x:\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpi = 150\n",
    "fig = plt.figure(dpi=dpi, figsize=(15, 30))\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.xaxis.grid(True)\n",
    "\n",
    "N = 60\n",
    "\n",
    "plt.barh(contries_x[:N], contries_y[:N], label='количество ретритов за все время по странам')\n",
    "plt.xticks(np.arange(0, 450, 25))\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig('ретриты по странам.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Book Retreats"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Товарная единица - ивент\n",
    "\n",
    "Нужно определить -\n",
    "- [ ] категория\n",
    "- [x] стоимость\n",
    "- [x] геопозиция\n",
    "- [x] дата\n",
    "- [x] длительность\n",
    "- [ ] количество людей (недоступно)\n",
    "- [ ] список услуг (недоступно)\n",
    "- [x] кто проводит (недоступно)\n",
    "- [x] сайт"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "page = 1\n",
    "# base_url='https://bookretreats.com/'\n",
    "base_url = 'https://bookretreats.com'\n",
    "# url = f'{base_url}/?page={page}'\n",
    "url = 'https://bookretreats.com/s/other-retreats?pageNumber=1'\n",
    "# url='https://bookretreats.com/7-day-detox-recharge-retreat-in-nature-sri-lanka'\n",
    "print(url)\n",
    "r = requests.get(url)\n",
    "response = r.text.encode('utf-8')\n",
    "\n",
    "soup = BeautifulSoup(response, 'html.parser')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "titles = soup.findAll('h3', {'class': 'text-xl text-gray-600 dark:text-gray-200 font-semibold mb-2'})\n",
    "titles = [title.text for title in titles]\n",
    "# soup.findAll('div', {'class': 'flex flex-col lg:justify-between h-full px-2 sm:p-0'})[0].text\n",
    "text = soup.findAll('a', {'class': 'col-span-12 sm:col-span-7'}, href=True)\n",
    "card_urls = []\n",
    "for t in text:\n",
    "    card_urls.append(t['href'])\n",
    "\n",
    "card_urls = [base_url + card for card in card_urls]\n",
    "card_urls"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "page = 1\n",
    "# base_url='https://bookretreats.com/'\n",
    "base_url = 'https://bookretreats.com'\n",
    "\n",
    "url = 'https://bookretreats.com/14-day-beginners-yoga-retreat-at-ayuryoga-eco-ashram-india'\n",
    "print(url)\n",
    "r = requests.get(url)\n",
    "response = r.text.encode('utf-8')\n",
    "\n",
    "soup = BeautifulSoup(response, 'html.parser')\n",
    "# soup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "soup.findAll('a', {'class': 'Button_textLink__2Ju0p Button_grayColor__2nH6m Button_mdSize__25L9h'})[1].text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "title = soup.findAll('div', {'class': 'col-span-4 sm:col-span-3 flex items-center'})[0].text\n",
    "print('title ', title)\n",
    "duration = soup.findAll('div', {'class': 'mb-4 text-base text-gray-500 dark:text-gray-300'})[0].text.split(' ')[0]\n",
    "print('duration ', duration)\n",
    "location = soup.findAll('a', {'class': 'Button_textLink__2Ju0p Button_grayColor__2nH6m Button_mdSize__25L9h'})[0].text\n",
    "print('location ', location)\n",
    "country = location.split(' ')[-1]\n",
    "print('county ', country)\n",
    "deposit = soup.findAll('div', {'class': 'tabular-nums text-xl text-gray-500 dark:text-gray-200 font-semibold'})[0].text[\n",
    "          1:].replace(',', '')\n",
    "print('deposit ', deposit)\n",
    "price = soup.findAll('div', {'class': 'tabular-nums text-2xl text-gray-500 dark:text-gray-200 font-extrabold'})[0].text[\n",
    "        1:].replace(',', '')\n",
    "print('price ', price)\n",
    "category = soup.findAll('a', {'class': 'inline text-sm leading-7 text-gray-500 dark:text-gray-400'})[1].text\n",
    "print('category ', category)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox(executable_path=r\"D:\\загрузки\\geckodriver-v0.30.0-win64\\geckodriver.exe\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "url = 'https://bookretreats.com/10-day-exploding-love-camp-in-akumal-riviera-maya-mexico?searchId=7518398'\n",
    "url = 'https://bookretreats.com/6-day-yogi-adventure-meditation-in-sintra-portugal?searchId=7792765'\n",
    "\n",
    "driver.get(url)\n",
    "print(url)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# soup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "elem = driver.find_element_by_xpath(\n",
    "    '//div[@class=\"Sidebar_sticky__5WOBf sticky top-24 mb-10\"]//span[contains(text(),\"Select Dates\")]')\n",
    "# elem.click()\n",
    "# driver.execute_script(\"arguments[0].click();\", elem)\n",
    "\n",
    "ActionChains(driver).move_to_element(elem).click().perform()\n",
    "dates = driver.find_elements_by_xpath('//div[@class=\"lg:max-h-72 overflow-scroll hide-scrollbars\"]')\n",
    "print(dates)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = driver.find_elements_by_xpath('//div[@class=\"lg:max-h-72 overflow-scroll hide-scrollbars\"]')\n",
    "x[0].text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "date =\n",
    "x[0].text.replace('2022', '2022;').replace('2023', '2023;').replace('2024', '2024;').replace('2025', '2025;').split(\n",
    "    ';')[:-1][1]\n",
    "\n",
    "raw_date = date.split(',')\n",
    "year = raw_date[-1].replace(' ', '')\n",
    "arrival, departure = raw_date[0].split('-')\n",
    "\n",
    "arrival_month = arrival.split(' ')[0]\n",
    "departure_month = departure.split(' ')[-2]\n",
    "\n",
    "if departure_month == '':\n",
    "    departure = arrival_month + departure\n",
    "else:\n",
    "    departure = departure[1:]\n",
    "\n",
    "arrival = arrival + year\n",
    "departure = departure + ' ' + year\n",
    "\n",
    "arrival = datetime.strptime(arrival, '%b %d %Y').date().isoformat()\n",
    "departure = datetime.strptime(departure, '%b %d %Y').date().isoformat()\n",
    "\n",
    "print('arrival ', arrival)\n",
    "print('departure ', departure)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'title': [0], 'duration': [0], 'arrival': [0], 'departure': [0],\n",
    "                     'organizer': [0], 'location': [0], 'country': [0], 'group_size': [0], 'category': [0],\n",
    "                     'price': [0], 'deposit': [0]})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data=pd.read_csv('bookretreats.csv')\n",
    "del data['Unnamed: 0']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start = 1\n",
    "stop = 1000\n",
    "\n",
    "base_url = 'https://bookretreats.com'\n",
    "page_category = 'wellness-retreats'\n",
    "driver = webdriver.Firefox(executable_path=r\"D:\\загрузки\\geckodriver-v0.30.0-win64\\geckodriver.exe\")\n",
    "driver.maximize_window()\n",
    "\n",
    "for page in range(start, stop):\n",
    "    url = f'https://bookretreats.com/s/{page_category}?pageNumber={page}'\n",
    "\n",
    "    print('-----------------')\n",
    "    print(f'i={page} new card page ', url)\n",
    "\n",
    "    r = requests.get(url)\n",
    "    response = r.text.encode('utf-8')\n",
    "\n",
    "    soup = BeautifulSoup(response, 'html.parser')\n",
    "    titles = soup.findAll('h3', {'class': 'text-xl text-gray-600 dark:text-gray-200 font-semibold mb-2'})\n",
    "    titles = [title.text for title in titles]\n",
    "\n",
    "    text = soup.findAll('a', {'class': 'col-span-12 sm:col-span-7'}, href=True)\n",
    "    card_urls = []\n",
    "    for t in text:\n",
    "        card_urls.append(t['href'])\n",
    "\n",
    "    card_urls = [base_url + card for card in card_urls]\n",
    "\n",
    "    for card_url in card_urls:\n",
    "        print(card_url)\n",
    "        r = requests.get(card_url)\n",
    "        response = r.text.encode('utf-8')\n",
    "        soup = BeautifulSoup(response, 'html.parser')\n",
    "        errors = []\n",
    "        #\n",
    "        try:\n",
    "            title = soup.findAll('div', {'class': 'col-span-4 sm:col-span-3 flex items-center'})[0].text\n",
    "        except Exception:\n",
    "            title = None\n",
    "            errors.append('title')\n",
    "\n",
    "        try:\n",
    "            duration = soup.findAll('div', {'class': 'mb-4 text-base text-gray-500 dark:text-gray-300'})[0].text.split(' ')[\n",
    "                0]\n",
    "        except Exception:\n",
    "            duration = None\n",
    "            errors.append('duration')\n",
    "\n",
    "        try:\n",
    "            location = soup.findAll('a', {'class': 'Button_textLink__2Ju0p Button_grayColor__2nH6m Button_mdSize__25L9h'})[\n",
    "                0].text\n",
    "            if 'reviews' in location:\n",
    "                location = soup.findAll('a', {'class': 'Button_textLink__2Ju0p Button_grayColor__2nH6m Button_mdSize__25L9h'})[\n",
    "                    1].text\n",
    "        except Exception:\n",
    "            location = None\n",
    "            errors.append('location')\n",
    "\n",
    "        try:\n",
    "            country = location.split(' ')[-1]\n",
    "        except Exception:\n",
    "            country = None\n",
    "            errors.append('country')\n",
    "\n",
    "        try:\n",
    "            deposit = soup.findAll('div', {'class': 'tabular-nums text-xl text-gray-500 dark:text-gray-200 font-semibold'})[\n",
    "                0].text[1:].replace(',', '')\n",
    "        except Exception:\n",
    "            deposit = None\n",
    "            errors.append('deposit')\n",
    "\n",
    "        try:\n",
    "            price = soup.findAll('div', {'class': 'tabular-nums text-2xl text-gray-500 dark:text-gray-200 font-extrabold'})[\n",
    "                0].text[1:].replace(',', '')\n",
    "        except Exception:\n",
    "            price = None\n",
    "            errors.append('price')\n",
    "        try:\n",
    "            category = soup.findAll('a', {'class': 'inline text-sm leading-7 text-gray-500 dark:text-gray-400'})[1].text\n",
    "        except Exception:\n",
    "            category = None\n",
    "            errors.append('category')\n",
    "\n",
    "        driver.get(card_url)\n",
    "\n",
    "        elem = None\n",
    "        try:\n",
    "            elem = driver.find_element_by_xpath(\n",
    "                '//div[@class=\"Sidebar_sticky__5WOBf sticky top-24 mb-10\"]//span[contains(text(),\"Select Dates\")]')\n",
    "        except Exception:\n",
    "            print('no button')\n",
    "            elem = False\n",
    "\n",
    "        if elem:\n",
    "\n",
    "            ActionChains(driver).move_to_element(elem).click().perform()\n",
    "\n",
    "            dates = driver.find_elements_by_xpath('//div[@class=\"lg:max-h-72 overflow-scroll hide-scrollbars\"]')\n",
    "            if len(dates) > 0:\n",
    "                dates = dates[0].text.replace('2022', '2022;').replace('2023', '2023;').replace('2024',\n",
    "                                                                                                '2024;').replace('2025',\n",
    "                                                                                                                 '2025;').split(\n",
    "                    ';')[:-1]\n",
    "                for date in dates:\n",
    "                    year = None\n",
    "                    arrival = None\n",
    "                    departure = None\n",
    "                    try:\n",
    "                        raw_date = date.split(',')\n",
    "                        year = raw_date[-1].replace(' ', '')\n",
    "                        arrival, departure = raw_date[0].split('-')\n",
    "\n",
    "                        arrival_month = arrival.split(' ')[0]\n",
    "                        departure_month = departure.split(' ')[-2]\n",
    "\n",
    "                        if departure_month == '':\n",
    "                            departure = arrival_month + departure\n",
    "                        else:\n",
    "                            departure = departure[1:]\n",
    "\n",
    "                        arrival = arrival + year\n",
    "                        departure = departure + ' ' + year\n",
    "\n",
    "                        arrival = datetime.strptime(arrival, '%b %d %Y').date().isoformat()\n",
    "                        departure = datetime.strptime(departure, '%b %d %Y').date().isoformat()\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "                    data = data.append(\n",
    "                        {'title': [title], 'duration': [duration], 'arrival': [arrival], 'departure': [departure],\n",
    "                         'organizer': [None], 'location': [location], 'country': [country], 'group_size': [None],\n",
    "                         'category': [category],\n",
    "                         'price': [price], 'deposit': [deposit]}, ignore_index=True)\n",
    "                data.to_csv('bookretreats.csv', encoding='utf-8')\n",
    "\n",
    "        # driver.refresh()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data=pd.read_csv('bookretreats.csv')\n",
    "del data['Unnamed: 0']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_np = data.values[1:]\n",
    "\n",
    "for i, line in enumerate(data_np):\n",
    "    title, duration, arrival, departure, organizer, location, country, group_size, category, price, deposit = line\n",
    "    title = title[2:-2]\n",
    "    duration = int(duration[2:-2])\n",
    "    arrival = arrival[2:-2]\n",
    "    departure = departure[2:-2]\n",
    "    organizer = None\n",
    "    location = location[2:-2]\n",
    "    country = country[2:-2]\n",
    "    group_size = None\n",
    "    category = category[2:-2]\n",
    "    price = int(price[2:-2])\n",
    "    deposit = int(deposit[2:-2])\n",
    "\n",
    "    data_np[i] = [title, duration, arrival, departure, organizer, location, country, group_size, category, price,\n",
    "                  deposit]\n",
    "\n",
    "cleaned_data = pd.DataFrame(\n",
    "    {'title': data_np[:, 0], 'duration': data_np[:, 1], 'arrival': data_np[:, 2], 'departure': data_np[:, 3],\n",
    "     'organizer': data_np[:, 4], 'location': data_np[:, 5], 'country': data_np[:, 6], 'group_size': data_np[:, 7],\n",
    "     'category': data_np[:, 8],\n",
    "     'price': data_np[:, 9], 'deposit': data_np[:, 10]})\n",
    "\n",
    "cleaned_data = cleaned_data.drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cleaned_data['duration'].astype(float).describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Количество полученных ретритов  (шт) - 3235\n",
    "\n",
    "Количество ретритов с привязкой к дате (шт): 2022г - 2370, 2023г - 457, 2024г - 141\n",
    "\n",
    "Количество ретритов без привязки к дате (шт): 38\n",
    "\n",
    "Количество уникальных ретритов с привязкой по дате по месяцам (шт): январь - 42, февраль - 49, март - 103, апрель - 182, май - 215, июнь - 186, июль - 166, август - 137, сентябрь - 180, октябрь - 159,  ноябрь - 113, декабрь - 92\n",
    "\n",
    "\n",
    "\n",
    "Цена (евро): среднее - 1061, медиана - 792,  дисперсия - 1014, миниум - 108, максимум - 8115\n",
    "\n",
    "Длительность (дни): среднее - 8.3, медиана - 7, дисперсия - 6.4, минимум - 2, максимум 29\n",
    "\n",
    "Количество человек (шт): нет информации\n",
    "\n",
    "Сумма стоимости ретритов х количество участников (млн евро) - нет информации"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Retreet centres"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "url = 'https://bookretreats.com/center/chteau-de-rhodes'\n",
    "# url = 'https://bookretreats.com/center/42-acres'\n",
    "print(url)\n",
    "r = requests.get(url)\n",
    "response = r.text\n",
    "soup = BeautifulSoup(response, 'html.parser')\n",
    "# soup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "imgURL = \"http://site.meishij.net/r/58/25/3568808/a3568808_142682562777944.jpg\"\n",
    "\n",
    "urllib.request.urlretrieve(imgURL, \"local-filename.jpg\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# страна и ссылка на tripadvisor\n",
    "name=soup.findAll('h1',{'class':'text-2xl sm:text-3xl font-bold my-0'})[0].text\n",
    "print(name)\n",
    "location=soup.findAll('a',{'class':'Button_textLink__ONaN7 Button_grayColor__DiCOL Button_mdSize__prgpT','href':'#location'})[0].text\n",
    "print(location)\n",
    "country=location.split()[-1]\n",
    "print(country)\n",
    "trip_advisor=soup.findAll('a',{'class':'Button_textLink__ONaN7 Button_grayColor__DiCOL Button_mdSize__prgpT','rel':'noopener noreferrer'})\n",
    "if len(trip_advisor)>0:\n",
    "    trip_advisor=trip_advisor[0]['href']\n",
    "else:\n",
    "    trip_advisor=None\n",
    "print(trip_advisor)\n",
    "loc_description=soup.findAll('div',{'class':'CollapsibleContent_content__Kc4nd'})[0].text\n",
    "print(loc_description[:200])\n",
    "\n",
    "cleaned_images_links=[]\n",
    "images_links=soup.findAll('img',{'class':'object-cover object-center'})\n",
    "for link in images_links:\n",
    "    if 'https' in link['src']:\n",
    "        cleaned_images_links.append(link['src'])\n",
    "\n",
    "print(cleaned_images_links)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "centers_data = pd.DataFrame({'title': [0], 'location': [0], 'country': [0], 'trip_advisor': [0], 'description':[0],'url': [0]})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "centers_data.to_csv('centers_bookretreats.csv', encoding='utf-8',index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "centers_data=pd.read_csv('centers_bookretreats.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cleaned_links=[]\n",
    "\n",
    "pages_number=327\n",
    "workers=50\n",
    "\n",
    "pages_step=int(pages_number/workers)\n",
    "diff=pages_number-pages_step*workers\n",
    "\n",
    "pages_start=[pages_step*i+1 for i in range(workers)]\n",
    "pages_end=[pages_step*i for i in range(1,workers+1)]\n",
    "pages_end[-1]+=diff\n",
    "\n",
    "async def get_links(start, end):\n",
    "    global cleaned_links\n",
    "    for i in range(start, end+1):\n",
    "        print(i)\n",
    "        url = f'https://bookretreats.com/centers/search?pageNumber={i}'\n",
    "\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.get(url) as response:\n",
    "\n",
    "                response = await response.text()\n",
    "                soup = BeautifulSoup(response, 'html.parser')\n",
    "\n",
    "                links=soup.findAll('a', {'class': 'sm:col-span-2 flex flex-col justify-between flex-grow p-2'})\n",
    "                for link in links:\n",
    "                    cleaned_links.append('https://bookretreats.com'+link['href'])\n",
    "                np.save('center_links.npy',cleaned_links)\n",
    "\n",
    "\n",
    "\n",
    "futures = [get_links(pages_start[i],pages_end[i]) for i in range(workers)]\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(asyncio.wait(futures))\n",
    "# loop.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pages_number=5783\n",
    "workers=100\n",
    "folder='centres_images'\n",
    "img_links=[]\n",
    "\n",
    "pages_step=int(pages_number/workers)\n",
    "diff=pages_number-pages_step*workers\n",
    "\n",
    "pages_start=[pages_step*i+1 for i in range(workers)]\n",
    "pages_end=[pages_step*i for i in range(1,workers+1)]\n",
    "pages_end[-1]+=diff\n",
    "\n",
    "cleaned_links=np.load('center_links.npy')\n",
    "\n",
    "\n",
    "async def get_center(start, end):\n",
    "    global centers_data\n",
    "    global img_links\n",
    "    global cleaned_links\n",
    "    for i in range(start, end+1):\n",
    "\n",
    "        center_url = cleaned_links[i]\n",
    "\n",
    "\n",
    "        async with aiohttp.ClientSession() as center_session:\n",
    "            async with center_session.get(center_url) as center_response:\n",
    "\n",
    "                center_response =  await center_response.text()\n",
    "                soup = BeautifulSoup(center_response, 'html.parser')\n",
    "\n",
    "                print(f'iter {i}/{end+1}')\n",
    "                # 1\n",
    "                name=soup.findAll('h1',{'class':'text-2xl sm:text-3xl font-bold my-0'})[0].text\n",
    "                # 2\n",
    "                location=soup.findAll('a',{'class':'Button_textLink__ONaN7 Button_grayColor__DiCOL Button_mdSize__prgpT','href':'#location'})[0].text\n",
    "                # 3\n",
    "                country=location.split(',')[-1][1:]\n",
    "                # 4\n",
    "                trip_advisor=soup.findAll('a',{'class':'Button_textLink__ONaN7 Button_grayColor__DiCOL Button_mdSize__prgpT','rel':'noopener noreferrer'})\n",
    "                if len(trip_advisor)>0:\n",
    "                    trip_advisor=trip_advisor[0]['href']\n",
    "                else:\n",
    "                    trip_advisor=None\n",
    "                # 5\n",
    "                loc_description=soup.findAll('div',{'class':'CollapsibleContent_content__Kc4nd'})[0].text\n",
    "\n",
    "\n",
    "                cleaned_images_links=[]\n",
    "                images_links=soup.findAll('img',{'class':'object-cover object-center'})\n",
    "                for link in images_links:\n",
    "                    if 'https' in link['src']:\n",
    "                        cleaned_images_links.append(link['src'])\n",
    "\n",
    "                img_links.append((name,cleaned_images_links))\n",
    "                np.save('img_links.npy', img_links)\n",
    "\n",
    "                # for i,link in enumerate(cleaned_images_links):\n",
    "                #\n",
    "                #     image = requests.get(link).content\n",
    "                #     with open(f'{folder}/{name}_{i}.jpg', 'wb') as handler:\n",
    "                #         handler.write(image)\n",
    "\n",
    "                # centers_data=pd.read_csv('centers_bookretreats.csv')\n",
    "\n",
    "                centers_data = centers_data.append({'title': name, 'location': location, 'country': country, 'trip_advisor': trip_advisor, 'description': loc_description,'url': center_url}, ignore_index=True)\n",
    "                centers_data.to_csv('centers_bookretreats.csv', encoding='utf-8',index=False)\n",
    "\n",
    "\n",
    "futures = [get_center(pages_start[i],pages_end[i]) for i in range(workers)]\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(asyncio.wait(futures))\n",
    "loop.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "centers_data.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cleaned_df=centers_data.dropna(subset=['url'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cleaned_df.to_csv('centers_bookretreats.csv', encoding='utf-8',index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cleaned_df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cleaned_df[cleaned_df.title=='Ayur Yoga School']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "centers_data[centers_data.title=='Ayur Yoga School']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "centers_data[centers_data.url==None].info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "contries=[]\n",
    "for line in centers_data['location']:\n",
    "    text=line.split(',')[-1]\n",
    "    if text[0]==' ':\n",
    "        text=text[1:]\n",
    "    contries.append(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "values = contries\n",
    "counts = Counter(values)\n",
    "contries_x = []\n",
    "contries_y = []\n",
    "for k, count in counts.most_common():\n",
    "    contries_x.append(k)\n",
    "    contries_y.append(count)\n",
    "    print(k)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "torch",
   "language": "python",
   "display_name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}