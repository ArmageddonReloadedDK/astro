{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import nest_asyncio\n",
    "from selenium import webdriver\n",
    "from loguru import logger\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Спарсим рамблер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это работает так: сначала код запрашивает требуемую страинцу у рамблера, тот в ответ отправляет html страницу, а затем идет поиск тега, в котором находится текст гороскопа. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signs=['aries','gemini','taurus','cancer','leo','virgo','libra','scorpio','sagittarius','capricorn','aquarius','pisces']\n",
    "data = pd.read_csv('data.csv', names=['date', 'sign', 'text'])\n",
    "\n",
    "for i in range(2004,2021):\n",
    "    for j in range(1,13):\n",
    "        for k in range(1,32):\n",
    "            for sign in signs:\n",
    "                try:\n",
    "                    year=i\n",
    "                    month=j\n",
    "                    day=k\n",
    "                    if month<10:\n",
    "                        month='0'+str(month)\n",
    "                    if day<10:\n",
    "                        day='0'+str(day)\n",
    "                    date='%s-%s-%s'%(year,month,day)\n",
    "                    url = 'https://horoscopes.rambler.ru/%s/%s/?updated'% (sign,date)  # url для второй страницы\n",
    "                    print(url)\n",
    "                    r = requests.get(url)\n",
    "                    response = r.text.encode('utf-8')\n",
    "\n",
    "                    soup = BeautifulSoup(response, features=\"lxml\")\n",
    "                    text = soup.find('div', {'class': '_1dQ3'}).text\n",
    "                    data=data.append({'date': date, 'sign': sign, 'text': text}, ignore_index=True)\n",
    "                    data.to_csv('data.csv', encoding = 'utf-8')\n",
    "                    print('saved date=', date, ' sign=', sign)\n",
    "                except Exception:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сайт Ростова-на-Дону"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.1rnd.ru/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news=[]\n",
    "url='https://www.1rnd.ru/news/3089599'\n",
    "print(url)\n",
    "r = requests.get(url)\n",
    "response = r.text\n",
    "soup = BeautifulSoup(response, 'html.parser')\n",
    "\n",
    "news_text=soup.findAll('p', {'style': \"text-align:justify;\"})\n",
    "date_text=soup.findAll('div', {'class': \"article-info__time\"})\n",
    "header_text=soup.findAll('div',{'class':'title-container inner-title'})\n",
    "\n",
    "if news_text:\n",
    "    print('success')\n",
    "    text_line=''\n",
    "    for text in news_text:\n",
    "         text_line=text_line+text.get_text()\n",
    "    news.append([header_text[0].get_text(),date_text[0].get_text().split(),text_line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.DataFrame({'header': [0], 'date': [0], 'news': [0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start1=7135\n",
    "start2=300000\n",
    "start3=600000\n",
    "start4=900000\n",
    "start5=1200000\n",
    "start6=1500000\n",
    "start7=1800000\n",
    "start8=2100000\n",
    "start9=2400000\n",
    "start10=2700000\n",
    "\n",
    "end1=299999\n",
    "end2=599999\n",
    "end3=899999\n",
    "end4=1199999\n",
    "end5=1499999\n",
    "end6=1799999\n",
    "end7=2099999\n",
    "end8=2399999\n",
    "end9=2699999\n",
    "end10=2999999\n",
    "\n",
    "\n",
    "async def get_news(start,end):\n",
    "    global news\n",
    "    for i in range(start,end):\n",
    "        url ='https://www.1rnd.ru/news/{0}'.format(str(i)) \n",
    "        print(url)\n",
    "        \n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.get(url) as response:\n",
    "\n",
    "                response = await response.text()\n",
    "                soup = BeautifulSoup(response, 'html.parser')\n",
    "\n",
    "                news_text=soup.findAll('p', {'style': \"text-align:justify;\"})\n",
    "                date_text=soup.findAll('div', {'class': \"article-info__time\"})\n",
    "                header_text=soup.findAll('div',{'class':'title-container inner-title'})\n",
    "\n",
    "                if news_text:\n",
    "                    print('success')\n",
    "                    text_line=''\n",
    "                    for text in news_text:\n",
    "                         text_line=text_line+text.get_text()\n",
    "                            \n",
    "                    date=date_text[0].get_text()\n",
    "                    header=header_text[0].get_text()\n",
    "\n",
    "                    news=news.append({'header': header, 'date': date, 'news': text_line}, ignore_index=True)\n",
    "                    news.to_csv('news.csv', encoding = 'utf-8')\n",
    "\n",
    "futures = [get_news(start1,end1),\n",
    "           get_news(start2,end2),\n",
    "           get_news(start3,end3),\n",
    "           get_news(start4,end4),\n",
    "           get_news(start5,end5),\n",
    "           get_news(start6,end6),\n",
    "           get_news(start7,end7),\n",
    "           get_news(start8,end8),\n",
    "           get_news(start9,end9),\n",
    "           get_news(start10,end10)]\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(asyncio.wait(futures))\n",
    "loop.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Товарная единица - ивент\n",
    "\n",
    "Нужно определить - \n",
    " - [x] категория\n",
    " - [x] стоимость \n",
    " - [x] геопозиция\n",
    " - [x] дата\n",
    " - [x] длительность\n",
    " - [x] количество людей\n",
    " - [ ] список услуг (недоступно)\n",
    " - [x] кто проводит\n",
    " - [x] сайт"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# BookYogaRetreats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "page=1\n",
    "base_url='www.bookyogaretreats.com'\n",
    "url = f'https://{base_url}/?page={page}'\n",
    "print(url)\n",
    "r = requests.get(url)\n",
    "response = r.text.encode('utf-8')\n",
    "\n",
    "soup = BeautifulSoup(response)\n",
    "text = soup.findAll('a', {'class': 'js-showcard-link'},href=True)\n",
    "card_urls=[]\n",
    "for t in text:\n",
    "    card_urls.append(t['href'])\n",
    "# data=data.append({'date': date, 'sign': sign, 'text': text}, ignore_index=True)\n",
    "# data.to_csv('data.csv', encoding = 'utf-8')\n",
    "# print('saved date=', date, ' sign=', sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page=1\n",
    "base_url='www.bookyogaretreats.com'\n",
    "url = f'https://{base_url}{card_urls[10]}'\n",
    "# url='https://www.bookyogaretreats.com/yoga-weeks/6-day-urban-yoga-retreat-in-barcelona-catalonia'\n",
    "print(url)\n",
    "r = requests.get(url)\n",
    "response = r.text.encode('utf-8')\n",
    "\n",
    "soup = BeautifulSoup(response,'html.parser')\n",
    "text = soup.findAll('a', {'class': 'js-showcard-link'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.findAll('div', {'class': 'pre-main-content__location'})[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.findAll('div', {'class': 'listing-title'})[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration=soup.findAll('div', {'class': 'subtitle-small listing-query-box__duration js-sc3-inquiry-modal-duration js-insert-duration'})[0].text\n",
    "duration=duration.replace('\\n','').split(' ')[0]\n",
    "duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title=soup.findAll('h1', {'class': 'listing-title__title title listing-title-new'})[0].text\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrival=soup.findAll('span', {'class': 'js-sc1-inquiry-modal-arrival'})\n",
    "if len(arrival)!=0:\n",
    "    arrival=arrival[0].text\n",
    "    arrival=arrival.replace(',','').split(' ')\n",
    "    arrival[1]=arrival[1][:3]\n",
    "    arrival=' '.join(arrival[1:])\n",
    "    arrival=datetime.strptime(arrival, '%b %d %Y').date()\n",
    "else:\n",
    "    arrival=None\n",
    "    \n",
    "\n",
    "print(arrival)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "departure=soup.findAll('span', {'class': 'js-sc1-inquiry-modal-departure'})\n",
    "if len(departure)!=0:\n",
    "    departure=departure[0].text\n",
    "    departure=departure.replace(',','').split(' ')\n",
    "    departure[1]=departure[1][:3]\n",
    "    departure=' '.join(departure[1:])\n",
    "    departure=datetime.strptime(departure, '%b %d %Y').date()\n",
    "else:\n",
    "    departure=None\n",
    "\n",
    "\n",
    "print(departure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.strptime(' 17 2022', '%b %d %Y').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organizer=soup.findAll('div', {'class': 'listing-organizer-card__content'})[0].text[20:].split('\\n')[0]\n",
    "organizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=soup.findAll('div', {'class': 'listing-overview__notes'})[0].text.split('\\n')\n",
    "arr=[ a for a in arr if a!='']\n",
    "duration=arr[0].split(' ')[0]\n",
    "duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location=soup.findAll('div', {'class': 'listing-title__location listing-location-new'})\n",
    "if len(location[0])!=0:\n",
    "    location=location[0].text\n",
    "else:\n",
    "    location=None\n",
    "\n",
    "print(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=soup.findAll('div', {'class': 'listing-overview__notes'})[0].text.split('\\n')\n",
    "arr=[ a for a in arr if a!='']\n",
    "group_size=[a for a in arr if 'Group' in a]\n",
    "\n",
    "if len(group_size)!=0:\n",
    "    if len(group_size[0].split(' '))==6:\n",
    "         group_size=group_size[0].split(' ')[4]\n",
    "    elif len(group_size[0].split(' '))==4:\n",
    "        group_size=group_size[0].split(' ')[2]\n",
    "else:\n",
    "    group_size=None\n",
    "\n",
    "print(arr)\n",
    "print(group_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox(executable_path= r\"D:\\загрузки\\geckodriver-v0.30.0-win64\\geckodriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)\n",
    "driver.find_elements_by_class_name('price')[0].text.split(' ')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main page\n",
    "\n",
    "main_page_url='https://www.bookyogaretreats.com'\n",
    "r = requests.get(main_page_url)\n",
    "response = r.text.encode('utf-8')\n",
    "\n",
    "soup = BeautifulSoup(response)\n",
    "\n",
    "categories_raw=soup.findAll('li', {'class': 'fakeCheck'})\n",
    "categories_raw=[' '.join(category.text.replace('\\n','').split(' ')[:-1]) for category in categories_raw][12:-4]\n",
    "categories=[]\n",
    "\n",
    "for i,category in enumerate(categories_raw):\n",
    "        if 'level' in category.lower():\n",
    "            categories.append(category.split(' ')[1])\n",
    "        else:\n",
    "            categories.append(category)\n",
    "\n",
    "# categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.findAll('div', {'id': 'error-page'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'https://www.bookyogaretreats.com/sr?c=budget-retreats&page=1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'title': [0],'duration': [0], 'arrival': [0], 'departure': [0], 'organizer': [0], 'location': [0], 'group_size': [0], 'price': [0]})\n",
    "# data_categories={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_page_url='https://www.bookyogaretreats.com'\n",
    "\n",
    "ua = UserAgent(verify_ssl=False)\n",
    "headers = {'User-Agent': ua.random}\n",
    "\n",
    "\n",
    "r = requests.get(main_page_url,headers=headers)\n",
    "response = r.text.encode('utf-8')\n",
    "soup = BeautifulSoup(response)\n",
    "\n",
    "categories_raw=soup.findAll('li', {'class': 'fakeCheck'})\n",
    "categories_raw=[' '.join(category.text.replace('\\n','').split(' ')[:-1]) for category in categories_raw][12:-4]\n",
    "categories=[]\n",
    "\n",
    "driver = webdriver.Firefox(executable_path= r\"D:\\загрузки\\geckodriver-v0.30.0-win64\\geckodriver.exe\")\n",
    "\n",
    "for i,category in enumerate(categories_raw):\n",
    "        if 'level' in category.lower():\n",
    "            categories.append(category.split(' ')[1])\n",
    "        else:\n",
    "            categories.append(category)\n",
    "\n",
    "\n",
    "# c_u=category_page_url=main_page_url+f'/sr?c={cat}&page={page}'\n",
    "            \n",
    "for page in range(428,429):\n",
    "#     cat='-'.join(category.split(' ')).lower()\n",
    "    cat='cat '\n",
    "#     stop=False\n",
    "\n",
    "#     data_categories[cat]=[]\n",
    "#     while stop is False:\n",
    "        \n",
    "    page_url=main_page_url+f'/?page={page}'\n",
    "    print('NEW Page ',page_url,'\\n----------------------')\n",
    "\n",
    "    # страница каталога \n",
    "    r = requests.get(page_url,headers=headers)\n",
    "    response = r.text.encode('utf-8')\n",
    "    soup = BeautifulSoup(response)\n",
    "\n",
    "    # если такой страницы нет, т.е. каталог закончился\n",
    "    error=soup.findAll('div', {'id': 'error-page'})\n",
    "\n",
    "    if len(error)>0:\n",
    "        stop=True\n",
    "        print(f'####Category page {page} ERROR####')\n",
    "        break\n",
    "\n",
    "    text = soup.findAll('a', {'class': 'js-showcard-link'},href=True)\n",
    "    card_urls=[]\n",
    "    for t in text:\n",
    "        card_urls.append(t['href'])\n",
    "\n",
    "    for card_url in card_urls:\n",
    "\n",
    "        url = f'{main_page_url}{card_url}'\n",
    "        print(f'category {cat}, url {url}')\n",
    "        r = requests.get(url,headers=headers)\n",
    "        response = r.text.encode('utf-8')\n",
    "        soup = BeautifulSoup(response,'html.parser')\n",
    "\n",
    "        error=soup.findAll('div', {'id': 'error-page'})\n",
    "        if len(error)>0:\n",
    "            continue\n",
    "\n",
    "\n",
    "        arr=soup.findAll('div', {'class': 'listing-overview__notes'})[0].text.split('\\n')\n",
    "        arr=[ a for a in arr if a!='']\n",
    "        duration=arr[0].split(' ')[0]\n",
    "\n",
    "        title=soup.findAll('h1', {'class': 'listing-title__title title listing-title-new'})[0].text\n",
    "#             data_categories[cat].append(title)\n",
    "\n",
    "        arrival=soup.findAll('span', {'class': 'js-sc1-inquiry-modal-arrival'})\n",
    "        if len(arrival)!=0:\n",
    "            arrival=arrival[0].text\n",
    "            arrival=arrival.replace(',','').split(' ')\n",
    "            arrival[1]=arrival[1][:3]\n",
    "            arrival=' '.join(arrival[1:])\n",
    "            arrival=datetime.strptime(arrival, '%b %d %Y').date()\n",
    "        else:\n",
    "            arrival=None\n",
    "\n",
    "        departure=soup.findAll('span', {'class': 'js-sc1-inquiry-modal-departure'})\n",
    "        if len(departure)!=0:\n",
    "            departure=departure[0].text\n",
    "            departure=departure.replace(',','').split(' ')\n",
    "            departure[1]=departure[1][:3]\n",
    "            departure=' '.join(departure[1:])\n",
    "            departure=datetime.strptime(departure, '%b %d %Y').date()\n",
    "        else:\n",
    "            departure=None\n",
    "\n",
    "        organizer=soup.findAll('div', {'class': 'listing-organizer-card__content'})[0].text[20:].split('\\n')[0]\n",
    "\n",
    "        location=soup.findAll('div', {'class': 'listing-title__location listing-location-new'})\n",
    "        if len(location[0])!=0:\n",
    "            location=location[0].text\n",
    "        else:\n",
    "            location=None\n",
    "\n",
    "        arr=soup.findAll('div', {'class': 'listing-overview__notes'})[0].text.split('\\n')\n",
    "        arr=[ a for a in arr if a!='']\n",
    "        group_size=[a for a in arr if 'Group' in a]\n",
    "\n",
    "        if len(group_size)!=0:\n",
    "            if len(group_size[0].split(' '))==6:\n",
    "                 group_size=group_size[0].split(' ')[4]\n",
    "            elif len(group_size[0].split(' '))==4:\n",
    "                group_size=group_size[0].split(' ')[2]\n",
    "        else:\n",
    "            group_size=None\n",
    "\n",
    "        driver.get(url)\n",
    "        \n",
    "        price_list=driver.find_elements_by_class_name('price')\n",
    "        if len(price_list)>0:\n",
    "            if len(price_list[0].text.split(' '))<2:\n",
    "                price=None\n",
    "            else:\n",
    "                price=price_list[0].text.split(' ')[1]\n",
    "        else:\n",
    "            price=None\n",
    "\n",
    "        data=data.append({'title': [title],'duration': [duration], 'arrival': [arrival], 'departure':[departure] ,\n",
    "                          'organizer': [organizer], 'location': [location], 'group_size': [group_size],\n",
    "                          'price': [price]}, ignore_index=True)\n",
    "    data.to_csv('data.csv', encoding = 'utf-8')\n",
    "#         np.save('data_categories.npy',data_categories)\n",
    "\n",
    "#         page+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Только страны"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_countries = pd.DataFrame({'title': [0], 'country': [0]})\n",
    "# data_categories={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_page_url='https://www.bookyogaretreats.com'\n",
    "\n",
    "ua = UserAgent(verify_ssl=False)\n",
    "headers = {'User-Agent': ua.random}\n",
    "\n",
    "for page in range(0,429):\n",
    "\n",
    "    cat='cat '\n",
    "        \n",
    "    page_url=main_page_url+f'/?page={page}'\n",
    "    print('NEW Page ',page_url,'\\n----------------------')\n",
    "\n",
    "    # страница каталога \n",
    "    r = requests.get(page_url,headers=headers)\n",
    "    response = r.text.encode('utf-8')\n",
    "    soup = BeautifulSoup(response)\n",
    "    \n",
    "    countries=[country.text for country in soup.findAll('div', {'class': 'pre-main-content__location'})]\n",
    "    titles=[title.text for title in soup.findAll('div', {'class': 'listing-title'})]\n",
    "    \n",
    "    for i,title in enumerate(titles):\n",
    "        \n",
    "        data_countries=data_countries.append({'title': title[1:-1], 'country': countries[i][1:-1]}, ignore_index=True)\n",
    "    data_countries.to_csv('data_countries.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_countries=data_countries.drop_duplicates()\n",
    "cleaned_countries=cleaned_countries.drop(cleaned_countries.index[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_countries.head()\n",
    "cleaned_countries.to_csv('cleaned_countries.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in cleaned_countries.values:\n",
    "    print(line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat='-'.join(categories[0].split(' '))\n",
    "category_page_url=main_page_url+f'/sr?c={cat}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paths=glob.glob('byr/*')\n",
    "dfs=[pd.read_csv(path) for path in df_paths]\n",
    "data=pd.concat(dfs)\n",
    "del data['Unnamed: 0']\n",
    "data_df=data.drop_duplicates()\n",
    "data=data_df.values[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paths=glob.glob('byr/*')\n",
    "dfs=[pd.read_csv(path) for path in df_paths]\n",
    "data=pd.concat(dfs)\n",
    "del data['Unnamed: 0']\n",
    "data_df=data.drop_duplicates()\n",
    "data=data_df.values[1:]\n",
    "\n",
    "\n",
    "for i,line in enumerate(data):\n",
    "\n",
    "    title,duration,arrival,departure,organizer,location,group_size,price=line\n",
    "    title=title[2:-2]\n",
    "    duration=float(duration[2:-2])\n",
    "    \n",
    "    if len(arrival)>6:\n",
    "        arrival=arrival[15:-2].replace(',','').replace(' ','-')\n",
    "    else:\n",
    "        arrival=None\n",
    "        \n",
    "    if len(departure)>6:\n",
    "        departure=departure[15:-2].replace(',','').replace(' ','-')\n",
    "    else:\n",
    "        departure=None\n",
    "        \n",
    "    organizer=organizer[2:-2]\n",
    "    location=location[2:-2]\n",
    "\n",
    "    if 'None' in group_size:\n",
    "        group_size=None\n",
    "    elif '-' in group_size:\n",
    "        group_size=group_size[2:-2].split('-')[1]\n",
    "        group_size=float(group_size)\n",
    "    else:\n",
    "        group_size=float(group_size[2:-2])\n",
    "\n",
    "    if 'None' in price:\n",
    "        price=None\n",
    "    else:\n",
    "        price=price[2:-2].replace(',','')\n",
    "        price=float(price)\n",
    "\n",
    "    \n",
    "    data[i]=[title,duration,arrival,departure,organizer,location,group_size,price]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = pd.DataFrame({'title': data[:,0],'duration': data[:,1], 'arrival': data[:,2], 'departure': data[:,3], 'organizer': data[:,4], 'location': data[:,5], 'group_size': data[:,6], 'price': data[:,7]})\n",
    "cleaned_data.to_csv('cleaned_data.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step=1\n",
    "year='2024'\n",
    "for line in cleaned_data.values:\n",
    "    if  line[2] is None:\n",
    "#         if line[2][:4]==year:\n",
    "            step+=1\n",
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data['price'].astype(float).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=0\n",
    "for line in  cleaned_data.values:\n",
    "    price=line[-1]\n",
    "    group_size=line[-2]\n",
    "    if price is not None and group_size is not None:\n",
    "        total+=price*group_size\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for  month_comp in range(1,13):\n",
    "    titles=[]\n",
    "    for line in cleaned_data.values:\n",
    "        if  line[2] is not None:\n",
    "            year,month,day=line[2].split('-')\n",
    "            title=line[0]\n",
    "            if str(month_comp)==month:\n",
    "                titles.append(title)\n",
    "\n",
    "    month_df=pd.DataFrame({'titles':titles}).drop_duplicates()\n",
    "\n",
    "    print(len(month_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество ретритов на сайте (шт) - 4650\n",
    "\n",
    "Количество ретритов с привязкой к дате (шт): 2022г - 1437, 2023г - 36, 2024г - 2\n",
    "\n",
    "Количество ретритов без привязки к дате (шт): 3181\n",
    "\n",
    "Количество уникальных ретритов с привязкой по дате по месяцам (шт): январь - 13, февраль - 64, март - 238, апрель - 246, май - 212, июнь - 183, июль - 143, август - 101, сентябрь - 99, ноябрь - 56, декабрь - 28 \n",
    "\n",
    "\n",
    "\n",
    "Цена (евро): среднее - 1243, медиана - 900,  дисперсия - 1190, миниум - 18, максимум - 27300 \n",
    "\n",
    "Длительность (дни): среднее - 10, медиана - 6, дисперсия - 16, минимум - 1, максимум 365\n",
    "\n",
    "Количество человек (шт): среднее - 13.5, медиана - 12, дисперсия - 12, минимум - 1, максимум 500\n",
    "\n",
    "Сумма стоимости ретритов х количество участников (млн евро) - 56.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data=pd.read_csv('bookyogaretreat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contries=[]\n",
    "for line in cleaned_countries.values:\n",
    "    cnt=line[1]\n",
    "    contries.append(cnt)\n",
    "contries.sort()\n",
    "#144\n",
    "contries=contries[0:]\n",
    "contries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = contries\n",
    "counts = Counter(values)\n",
    "contries_x=[]\n",
    "contries_y=[]\n",
    "for k, count in counts.most_common():\n",
    "    contries_x.append(k)\n",
    "    contries_y.append(count)\n",
    "#     print(k, count * 'x')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=30\n",
    "plt.figure(figsize=(30,15))\n",
    "plt.plot(contries_x[:N],contries_y[:N])\n",
    "plt.yticks(np.arange(0,450,25))\n",
    "plt.grid(True)\n",
    "plt.legend(['количество ретритов за все время по странам'],fontsize=20)\n",
    "plt.savefig('ретриты по странам.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in contries_y:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpi = 150\n",
    "fig = plt.figure(dpi = dpi, figsize = (15,30 ))\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.xaxis.grid(True)\n",
    "\n",
    "N=60\n",
    "\n",
    "\n",
    "plt.barh(contries_x[:N], contries_y[:N], label='количество ретритов за все время по странам')\n",
    "plt.xticks(np.arange(0,450,25))\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig('ретриты по странам.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Book Retreats"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "page=1\n",
    "# base_url='https://bookretreats.com/'\n",
    "base_url='https://bookretreats.com'\n",
    "# url = f'{base_url}/?page={page}'\n",
    "url = 'https://bookretreats.com/s/other-retreats?pageNumber=1'\n",
    "print(url)\n",
    "r = requests.get(url)\n",
    "response = r.text.encode('utf-8')\n",
    "\n",
    "soup = BeautifulSoup(response,'html.parser')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "soup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "titles=soup.findAll('h3', {'class': 'text-xl text-gray-600 dark:text-gray-200 font-semibold mb-2'})\n",
    "titles=[title.text for title in titles]\n",
    "# soup.findAll('div', {'class': 'flex flex-col lg:justify-between h-full px-2 sm:p-0'})[0].text\n",
    "text = soup.findAll('a', {'class': 'col-span-12 sm:col-span-7'},href=True)\n",
    "card_urls=[]\n",
    "for t in text:\n",
    "    card_urls.append(t['href'])\n",
    "\n",
    "card_urls=[base_url+card for card in card_urls]\n",
    "card_urls"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}